# -*- coding: utf-8 -*-
"""
TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ Webã‚¢ãƒ—ãƒª (Streamlit)

èµ·å‹•æ–¹æ³•:
  streamlit run keyword_search_app.py

æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:
  A) ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿ -- PCã®PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆé…ã„ãŒç¢ºå®Ÿï¼‰
  B) ãƒ­ãƒ¼ã‚«ãƒ«JSON     -- â‘¥ã§äº‹å‰æŠ½å‡ºã—ãŸJSONã§é«˜é€Ÿæ¤œç´¢
  C) ã‚¯ãƒ©ã‚¦ãƒ‰         -- GitHub Pagesã®JSONã§æ¤œç´¢ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰

PDFã®é–²è¦§ï¼ˆå…¨ãƒ¢ãƒ¼ãƒ‰å…±é€šã§å„è¡Œã«ã€Œé–‹ãã€ãƒªãƒ³ã‚¯ï¼‰:
  ãƒ­ãƒ¼ã‚«ãƒ« â†’ localhostçµŒç”±ã§ãƒ–ãƒ©ã‚¦ã‚¶è¡¨ç¤ºï¼ˆæœŸé–“ç„¡åˆ¶é™ï¼‰
  ã‚¯ãƒ©ã‚¦ãƒ‰ â†’ TDnetãƒªãƒ³ã‚¯ã§ãƒ–ãƒ©ã‚¦ã‚¶è¡¨ç¤ºï¼ˆç´„30æ—¥ï¼‰
"""

import os
import re
import json
import socket
import threading
import datetime
import unicodedata
from http.server import HTTPServer, SimpleHTTPRequestHandler
from functools import partial
from urllib.parse import quote

import pandas as pd
import streamlit as st

try:
    import requests as _requests
except ImportError:
    _requests = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

# ============================================================
# è¨­å®š
# ============================================================
DEFAULT_PDF_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"
DEFAULT_TEXT_JSON_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "text_data")
GITHUB_PAGES_TEXT_BASE = "https://onokazu777.github.io/tdnet-viewer/data/text"
PRIORITY_KEYWORDS = ["äº‹æ¥­è¨ˆç”»", "äºˆæƒ³ã®ä¿®æ­£", "æ±ºç®—çŸ­ä¿¡", "èª¬æ˜è³‡æ–™", "æœˆæ¬¡", "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡"]


# ============================================================
# ãƒ­ãƒ¼ã‚«ãƒ«PDFé…ä¿¡ã‚µãƒ¼ãƒãƒ¼
# ============================================================
class _SilentHandler(SimpleHTTPRequestHandler):
    """ãƒ­ã‚°å‡ºåŠ›ã‚’æŠ‘åˆ¶ã—ãŸHTTPãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def log_message(self, format, *args):
        pass  # æ¨™æº–å‡ºåŠ›ã«ãƒ­ã‚°ã‚’å‡ºã•ãªã„


def _find_free_port() -> int:
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.bind(("localhost", 0))
        return s.getsockname()[1]


def start_local_pdf_server(root_dir: str) -> int:
    """ãƒ­ãƒ¼ã‚«ãƒ«PDFã‚’é…ä¿¡ã™ã‚‹HTTPã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã—ã€ãƒãƒ¼ãƒˆç•ªå·ã‚’è¿”ã™"""
    if "pdf_server_port" in st.session_state:
        return st.session_state["pdf_server_port"]

    port = _find_free_port()
    handler = partial(_SilentHandler, directory=root_dir)
    server = HTTPServer(("localhost", port), handler)
    thread = threading.Thread(target=server.serve_forever, daemon=True)
    thread.start()
    st.session_state["pdf_server_port"] = port
    return port


def local_pdf_url(port: int, date_str: str, pdf_filename: str) -> str:
    """ãƒ­ãƒ¼ã‚«ãƒ«PDFã®HTTP URL ã‚’æ§‹ç¯‰"""
    return f"http://localhost:{port}/{quote(date_str)}/{quote(pdf_filename)}"


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def norm_key(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()


def get_category(title: str) -> str:
    for kw in PRIORITY_KEYWORDS:
        if kw in title:
            return kw
    return "ãã®ä»–"


def extract_code_from_pdf_filename(pdf_filename: str) -> str:
    m = re.match(r"^([0-9A-Za-z]{4})_", str(pdf_filename))
    return m.group(1).upper() if m else ""


def list_date_folders(root_path: str) -> list[str]:
    if not os.path.isdir(root_path):
        return []
    return sorted([
        d for d in os.listdir(root_path)
        if os.path.isdir(os.path.join(root_path, d)) and re.fullmatch(r"\d{8}", d)
    ])


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ A: ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿
# ============================================================
def load_tdnet_meta(root_path: str, date_str: str) -> dict:
    day_csv = os.path.join(root_path, date_str, f"TDnet_Sorted_{date_str}.csv")
    root_csv = os.path.join(root_path, f"TDnet_Sorted_{date_str}.csv")
    csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None
    if csv_path is None:
        return {}
    df = pd.read_csv(csv_path, dtype=str).fillna("")
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    if "PDFãƒ•ã‚¡ã‚¤ãƒ«å" not in df.columns:
        return {}
    index = {}
    for _, r in df.iterrows():
        pdf_key = norm_key(r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", ""))
        if not pdf_key:
            continue
        title_link = str(r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")).strip()
        display_text = str(r.get("ä¼šç¤¾å", "")).strip()
        m = re.match(r'=HYPERLINK\("([^"]*)",\s*"([^"]*)"\)', title_link)
        url = ""
        if m:
            url = m.group(1)
            display_text = m.group(2) or display_text
        bunrui = str(r.get("åˆ†é¡", "")).strip()
        if not bunrui:
            bunrui = get_category(display_text)
        index[pdf_key] = {
            "ä¼šç¤¾å": str(r.get("ä¼šç¤¾å", "")).strip(),
            "ã‚³ãƒ¼ãƒ‰": str(r.get("ã‚³ãƒ¼ãƒ‰", "")).strip()[:4],
            "åˆ†é¡": bunrui,
            "è¡¨é¡Œ": display_text,
            "URL": url or str(r.get("URLï¼ˆç”Ÿï¼‰", "")).strip(),
        }
    return index


def search_pdfs_local(
    root_path: str, date_from: str, date_to: str, keywords: list[str],
    pdf_server_port: int = 0, progress_callback=None,
) -> pd.DataFrame:
    all_dates = list_date_folders(root_path)
    target_dates = [d for d in all_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()
    total_pdfs = 0
    date_pdfs: dict[str, list[str]] = {}
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        pdfs = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]
        date_pdfs[d] = pdfs
        total_pdfs += len(pdfs)
    if total_pdfs == 0:
        return pd.DataFrame()

    results = []
    processed = 0
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        meta_index = load_tdnet_meta(root_path, d)
        for pdf_name in sorted(date_pdfs[d]):
            processed += 1
            pdf_path = os.path.join(day_dir, pdf_name)
            try:
                doc = fitz.open(pdf_path)
                kw_pages = {kw: set() for kw in keywords}
                for page_index, page in enumerate(doc, start=1):
                    text = page.get_text("text")
                    for kw in keywords:
                        if kw in text:
                            kw_pages[kw].add(page_index)
                doc.close()
                kw_result = {kw: " ".join(str(p) for p in sorted(pages)) for kw, pages in kw_pages.items()}
            except Exception:
                kw_result = {kw: "" for kw in keywords}

            if any(v for v in kw_result.values()):
                code = extract_code_from_pdf_filename(pdf_name)
                pdf_key = norm_key(pdf_name)
                meta = meta_index.get(pdf_key, {})
                pdf_url = local_pdf_url(pdf_server_port, d, pdf_name) if pdf_server_port else ""
                tdnet_url = meta.get("URL", "")  # TDnetãƒªãƒ³ã‚¯ï¼ˆCSVå‡ºåŠ›ç”¨ï¼‰
                row = {
                    "æ—¥ä»˜": d, "ã‚³ãƒ¼ãƒ‰": code,
                    "ä¼æ¥­å": meta.get("ä¼šç¤¾å", ""),
                    "åˆ†é¡": meta.get("åˆ†é¡", "ãã®ä»–"),
                    "PDF": pdf_url,
                    "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": pdf_path,
                    "TDnet_URL": tdnet_url,
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)
            if progress_callback:
                progress_callback(processed, total_pdfs)
    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ B/C: JSONçµŒç”±æ¤œç´¢
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_text_index_remote() -> list[str]:
    url = f"{GITHUB_PAGES_TEXT_BASE}/index.json"
    try:
        resp = _requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.json().get("dates", [])
    except Exception:
        return []


def list_text_json_dates_local(text_dir: str) -> list[str]:
    if not os.path.isdir(text_dir):
        return []
    return sorted([
        m.group(1) for fn in os.listdir(text_dir)
        if (m := re.match(r"text_(\d{8})\.json$", fn))
    ])


@st.cache_data(ttl=600, show_spinner=False)
def load_text_json_remote(date_str: str) -> dict:
    url = f"{GITHUB_PAGES_TEXT_BASE}/text_{date_str}.json"
    try:
        resp = _requests.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception:
        return {}


def load_text_json_local(text_dir: str, date_str: str) -> dict:
    path = os.path.join(text_dir, f"text_{date_str}.json")
    if not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def search_text_json(
    date_from: str, date_to: str, keywords: list[str],
    available_dates: list[str], load_func,
    pdf_server_port: int = 0,
    pdf_root: str = "",
    progress_callback=None,
) -> pd.DataFrame:
    """JSONçµŒç”±ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ã€‚pdf_server_port > 0 ãªã‚‰ãƒ­ãƒ¼ã‚«ãƒ«URLã€0ãªã‚‰TDnet URLã€‚"""
    target_dates = [d for d in available_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()
    results = []
    total_dates = len(target_dates)
    for idx, d in enumerate(target_dates):
        data = load_func(d)
        if not data or "files" not in data:
            if progress_callback:
                progress_callback(idx + 1, total_dates)
            continue
        for file_info in data["files"]:
            pages = file_info.get("pages", [])
            kw_result = {}
            for kw in keywords:
                hit_pages = []
                for page_idx, page_text in enumerate(pages, start=1):
                    if kw in page_text:
                        hit_pages.append(str(page_idx))
                kw_result[kw] = " ".join(hit_pages)

            if any(v for v in kw_result.values()):
                pdf_name = file_info.get("pdf", "")
                tdnet_url = file_info.get("url", "")  # TDnetãƒªãƒ³ã‚¯ï¼ˆå¸¸ã«ä¿æŒï¼‰
                if pdf_server_port and pdf_name:
                    pdf_url = local_pdf_url(pdf_server_port, d, pdf_name)
                    local_path = os.path.join(pdf_root, d, pdf_name) if pdf_root else ""
                else:
                    pdf_url = tdnet_url  # ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ¼ãƒ‰: TDnet URLã‚’è¡¨ç¤ºç”¨ã«ä½¿ã†
                    local_path = ""
                row = {
                    "æ—¥ä»˜": d,
                    "ã‚³ãƒ¼ãƒ‰": file_info.get("code", ""),
                    "ä¼æ¥­å": file_info.get("company", ""),
                    "åˆ†é¡": file_info.get("category", "ãã®ä»–"),
                    "PDF": pdf_url,
                    "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": local_path,
                    "TDnet_URL": tdnet_url,
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)
        if progress_callback:
            progress_callback(idx + 1, total_dates)
    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# Streamlit UI
# ============================================================
def main():
    st.set_page_config(page_title="TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", page_icon="ğŸ”", layout="wide")
    st.title("TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    st.caption("TDneté©æ™‚é–‹ç¤ºPDFã‹ã‚‰ã€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

    # ----- ã‚µã‚¤ãƒ‰ãƒãƒ¼ -----
    with st.sidebar:
        st.header("æ¤œç´¢æ¡ä»¶")

        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            options=[
                "ãƒ­ãƒ¼ã‚«ãƒ«PDFï¼ˆç›´æ¥æ¤œç´¢ï¼‰",
                "ãƒ­ãƒ¼ã‚«ãƒ«JSONï¼ˆé«˜é€Ÿæ¤œç´¢ï¼‰",
                "ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰",
            ],
            index=0,
            help=(
                "ãƒ­ãƒ¼ã‚«ãƒ«PDF: PCã®PDFã‚’ç›´æ¥æ¤œç´¢ï¼ˆé…ã„ãŒç¢ºå®Ÿï¼‰\n"
                "ãƒ­ãƒ¼ã‚«ãƒ«JSON: â‘¥ã§äº‹å‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã§é«˜é€Ÿæ¤œç´¢\n"
                "ã‚¯ãƒ©ã‚¦ãƒ‰: GitHub Pagesã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œç´¢ï¼ˆPDFä¸è¦ï¼‰"
            ),
        )
        is_local_pdf = "ãƒ­ãƒ¼ã‚«ãƒ«PDF" in data_source
        is_local_json = "ãƒ­ãƒ¼ã‚«ãƒ«JSON" in data_source
        is_cloud = "ã‚¯ãƒ©ã‚¦ãƒ‰" in data_source
        is_local = is_local_pdf or is_local_json

        pdf_root = ""
        text_json_dir = DEFAULT_TEXT_JSON_DIR

        if is_local_pdf:
            pdf_root = st.text_input("PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹", value=DEFAULT_PDF_ROOT)
            available_dates = list_date_folders(pdf_root)
            if not available_dates:
                st.warning(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_root}")
                st.stop()
        elif is_local_json:
            pdf_root = st.text_input("PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹", value=DEFAULT_PDF_ROOT)
            text_json_dir = st.text_input("ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ•ã‚©ãƒ«ãƒ€", value=DEFAULT_TEXT_JSON_DIR)
            available_dates = list_text_json_dates_local(text_json_dir)
            if not available_dates:
                st.warning(f"ãƒ†ã‚­ã‚¹ãƒˆJSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {text_json_dir}\n\nâ‘¥ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()
        else:
            with st.spinner("åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèªä¸­..."):
                available_dates = fetch_text_index_remote()
            if not available_dates:
                st.warning("ã‚¯ãƒ©ã‚¦ãƒ‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\nGitHub Actions â†’ Run workflow ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

        st.info(f"åˆ©ç”¨å¯èƒ½: {available_dates[0]} ã€œ {available_dates[-1]}ï¼ˆ{len(available_dates)}æ—¥åˆ†ï¼‰")

        min_date = datetime.datetime.strptime(available_dates[0], "%Y%m%d").date()
        max_date = datetime.datetime.strptime(available_dates[-1], "%Y%m%d").date()
        col1, col2 = st.columns(2)
        with col1:
            date_from = st.date_input("é–‹å§‹æ—¥", value=max_date, min_value=min_date, max_value=max_date)
        with col2:
            date_to = st.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date)

        st.divider()
        st.subheader("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å€‹ï¼‰")
        keywords_input = []
        default_keywords = ["å¢—ç”£", "ä¸Šæ–¹ä¿®æ­£", "ã‚·ã‚§ã‚¢æ‹¡å¤§", "ä¾¡æ ¼æ”¹å®š", "éœ€è¦å›å¾©"]
        for i in range(5):
            kw = st.text_input(
                f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}",
                value=default_keywords[i] if i < len(default_keywords) else "",
                key=f"kw_{i}",
                label_visibility="collapsed" if i > 0 else "visible",
                placeholder=f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}ï¼ˆç©ºæ¬„ã¯ç„¡è¦–ï¼‰",
            )
            if kw.strip():
                keywords_input.append(kw.strip())

        st.divider()
        search_clicked = st.button("æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True)
        if keywords_input:
            st.caption(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_input)}")
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # ----- ãƒ­ãƒ¼ã‚«ãƒ«PDFé…ä¿¡ã‚µãƒ¼ãƒãƒ¼èµ·å‹• -----
    pdf_server_port = 0
    if is_local and pdf_root:
        pdf_server_port = start_local_pdf_server(pdf_root)

    # ----- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: æ¤œç´¢å®Ÿè¡Œ -----
    if search_clicked and keywords_input:
        d_from = date_from.strftime("%Y%m%d")
        d_to = date_to.strftime("%Y%m%d")
        if d_from > d_to:
            st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        progress_bar = st.progress(0, text="æ¤œç´¢ä¸­...")

        if is_local_pdf:
            if fitz is None:
                st.error("PyMuPDF ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install pymupdf`")
                st.stop()
            def cb(c, t): progress_bar.progress(c / t if t else 0, text=f"PDFæ¤œç´¢ä¸­... ({c}/{t})")
            df = search_pdfs_local(pdf_root, d_from, d_to, keywords_input,
                                   pdf_server_port=pdf_server_port, progress_callback=cb)
        elif is_local_json:
            def cb(c, t): progress_bar.progress(c / t if t else 0, text=f"ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ä¸­... ({c}/{t}æ—¥)")
            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=lambda d: load_text_json_local(text_json_dir, d),
                pdf_server_port=pdf_server_port, pdf_root=pdf_root, progress_callback=cb,
            )
        else:
            def cb(c, t): progress_bar.progress(c / t if t else 0, text=f"ã‚¯ãƒ©ã‚¦ãƒ‰èª­ã¿è¾¼ã¿ä¸­... ({c}/{t}æ—¥)")
            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=load_text_json_remote,
                pdf_server_port=0,  # ã‚¯ãƒ©ã‚¦ãƒ‰ã¯TDnet URL
                progress_callback=cb,
            )

        progress_bar.empty()
        st.session_state["search_results"] = df
        st.session_state["search_keywords"] = keywords_input

    # ----- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢: çµæœè¡¨ç¤ºï¼ˆå…¨ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰ -----
    df = st.session_state.get("search_results")
    keywords_display = st.session_state.get("search_keywords", [])

    if df is not None:
        if df.empty:
            st.info("ãƒ’ãƒƒãƒˆã™ã‚‹PDFã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            st.subheader("æ¤œç´¢çµæœ")

            # åˆ†é¡ãƒ•ã‚£ãƒ«ã‚¿
            all_categories = sorted(df["åˆ†é¡"].unique().tolist())
            selected_categories = st.multiselect(
                "åˆ†é¡ã§ãƒ•ã‚£ãƒ«ã‚¿", options=all_categories, default=all_categories,
            )
            filtered_df = df[df["åˆ†é¡"].isin(selected_categories)] if selected_categories else df
            st.metric("ãƒ’ãƒƒãƒˆæ•°", f"{len(filtered_df)} ä»¶ / å…¨ {len(df)} ä»¶")

            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€ç•ªä¸Šï¼‰- BOMä»˜ãUTF-8ã§Excelå¯¾å¿œ
            csv_export = filtered_df.copy()
            # CSVç”¨ã®æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
            csv_export["æ—¥ä»˜"] = csv_export["æ—¥ä»˜"].apply(
                lambda x: f"{x[:4]}/{x[4:6]}/{x[6:]}" if len(str(x)) == 8 else x
            )
            # CSVç”¨: Excelã®HYPERLINKé–¢æ•°ã§ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãªãƒªãƒ³ã‚¯ã«ã™ã‚‹
            # TDnet URLå„ªå…ˆï¼ˆä¸€èˆ¬å…¬é–‹å‘ã‘ï¼‰ã€ç„¡ã‘ã‚Œã°ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹
            def _make_hyperlink(row):
                tdnet_url = row.get("TDnet_URL", "")
                local_path = row.get("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹", "")
                if tdnet_url and tdnet_url.startswith("http"):
                    return f'=HYPERLINK("{tdnet_url}","é–‹ã")'
                elif local_path:
                    return f'=HYPERLINK("{local_path}","é–‹ã")'
                return ""
            csv_export["PDF"] = csv_export.apply(_make_hyperlink, axis=1)
            csv_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡", "PDF"] + keywords_display
            csv_export = csv_export[[c for c in csv_cols if c in csv_export.columns]]
            # BOMä»˜ãUTF-8ã§ãƒã‚¤ãƒˆåˆ—ã¨ã—ã¦ç”Ÿæˆ
            csv_bytes = csv_export.to_csv(index=False).encode("utf-8-sig")
            st.download_button(
                label="çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_bytes,
                file_name=f"keyword_search_{date_from.strftime('%Y%m%d')}_{date_to.strftime('%Y%m%d')}.csv",
                mime="text/csv",
            )

            # è¡¨ç¤ºç”¨DataFrameï¼ˆå…¨ãƒ¢ãƒ¼ãƒ‰å…±é€šï¼‰
            display_df = filtered_df.copy().reset_index(drop=True)
            display_df["æ—¥ä»˜"] = display_df["æ—¥ä»˜"].apply(
                lambda x: f"{x[:4]}/{x[4:6]}/{x[6:]}" if len(str(x)) == 8 else x
            )

            table_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡", "PDF"] + keywords_display
            table_df = display_df[[c for c in table_cols if c in display_df.columns]]

            st.dataframe(
                table_df,
                use_container_width=True,
                hide_index=True,
                height=min(len(table_df) * 40 + 40, 600),
                column_config={
                    "PDF": st.column_config.LinkColumn(
                        "PDF",
                        display_text="é–‹ã",
                    ),
                },
            )

            if is_cloud:
                st.caption("â€» TDnetã®PDFãƒªãƒ³ã‚¯ã¯å…¬é–‹ã‹ã‚‰ç´„30æ—¥ã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

    elif df is None:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æœŸé–“ã‚’è¨­å®šã—ã€ã€Œæ¤œç´¢é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
