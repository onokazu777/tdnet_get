# -*- coding: utf-8 -*-
"""
TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ Webã‚¢ãƒ—ãƒª (Streamlit)

èµ·å‹•æ–¹æ³•:
  streamlit run keyword_search_app.py

æ¤œç´¢ãƒ¢ãƒ¼ãƒ‰:
  A) ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿ -- PCã®PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆé…ã„ãŒç¢ºå®Ÿï¼‰
  B) ãƒ­ãƒ¼ã‚«ãƒ«JSON     -- â‘¥ã§äº‹å‰æŠ½å‡ºã—ãŸJSONã§é«˜é€Ÿæ¤œç´¢
  C) ã‚¯ãƒ©ã‚¦ãƒ‰         -- GitHub Pagesã®JSONã§æ¤œç´¢ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰

PDFã®é–²è¦§:
  å…¨ãƒ¢ãƒ¼ãƒ‰å…±é€šã§TDnetã®PDFãƒªãƒ³ã‚¯ã‚’è¡¨ç¤ºã€‚ã‚¯ãƒªãƒƒã‚¯ã§ãƒ–ãƒ©ã‚¦ã‚¶ã«PDFãŒé–‹ãã€‚
"""

import os
import re
import json
import datetime
import unicodedata
import pandas as pd
import streamlit as st

try:
    import requests as _requests
except ImportError:
    _requests = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

# ============================================================
# è¨­å®š
# ============================================================
DEFAULT_PDF_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"
DEFAULT_TEXT_JSON_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "text_data")
GITHUB_PAGES_TEXT_BASE = "https://onokazu777.github.io/tdnet-viewer/data/text"
PRIORITY_KEYWORDS = ["äº‹æ¥­è¨ˆç”»", "äºˆæƒ³ã®ä¿®æ­£", "æ±ºç®—çŸ­ä¿¡", "èª¬æ˜è³‡æ–™", "æœˆæ¬¡", "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡"]


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def norm_key(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()


def get_category(title: str) -> str:
    for kw in PRIORITY_KEYWORDS:
        if kw in title:
            return kw
    return "ãã®ä»–"


def extract_code_from_pdf_filename(pdf_filename: str) -> str:
    m = re.match(r"^([0-9A-Za-z]{4})_", str(pdf_filename))
    return m.group(1).upper() if m else ""


def list_date_folders(root_path: str) -> list[str]:
    if not os.path.isdir(root_path):
        return []
    return sorted([
        d for d in os.listdir(root_path)
        if os.path.isdir(os.path.join(root_path, d)) and re.fullmatch(r"\d{8}", d)
    ])


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ A: ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿
# ============================================================
def load_tdnet_meta(root_path: str, date_str: str) -> dict:
    day_csv = os.path.join(root_path, date_str, f"TDnet_Sorted_{date_str}.csv")
    root_csv = os.path.join(root_path, f"TDnet_Sorted_{date_str}.csv")
    csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None
    if csv_path is None:
        return {}

    df = pd.read_csv(csv_path, dtype=str).fillna("")
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    if "PDFãƒ•ã‚¡ã‚¤ãƒ«å" not in df.columns:
        return {}

    index = {}
    for _, r in df.iterrows():
        pdf_key = norm_key(r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", ""))
        if not pdf_key:
            continue
        title_link = str(r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")).strip()
        display_text = str(r.get("ä¼šç¤¾å", "")).strip()
        m = re.match(r'=HYPERLINK\("([^"]*)",\s*"([^"]*)"\)', title_link)
        url = ""
        if m:
            url = m.group(1)
            display_text = m.group(2) or display_text
        bunrui = str(r.get("åˆ†é¡", "")).strip()
        if not bunrui:
            bunrui = get_category(display_text)
        index[pdf_key] = {
            "ä¼šç¤¾å": str(r.get("ä¼šç¤¾å", "")).strip(),
            "ã‚³ãƒ¼ãƒ‰": str(r.get("ã‚³ãƒ¼ãƒ‰", "")).strip()[:4],
            "åˆ†é¡": bunrui,
            "è¡¨é¡Œ": display_text,
            "URL": url or str(r.get("URLï¼ˆç”Ÿï¼‰", "")).strip(),
        }
    return index


def search_pdfs_local(
    root_path: str, date_from: str, date_to: str, keywords: list[str], progress_callback=None,
) -> pd.DataFrame:
    all_dates = list_date_folders(root_path)
    target_dates = [d for d in all_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    total_pdfs = 0
    date_pdfs: dict[str, list[str]] = {}
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        pdfs = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]
        date_pdfs[d] = pdfs
        total_pdfs += len(pdfs)
    if total_pdfs == 0:
        return pd.DataFrame()

    results = []
    processed = 0
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        meta_index = load_tdnet_meta(root_path, d)
        for pdf_name in sorted(date_pdfs[d]):
            processed += 1
            pdf_path = os.path.join(day_dir, pdf_name)
            try:
                doc = fitz.open(pdf_path)
                kw_pages = {kw: set() for kw in keywords}
                for page_index, page in enumerate(doc, start=1):
                    text = page.get_text("text")
                    for kw in keywords:
                        if kw in text:
                            kw_pages[kw].add(page_index)
                doc.close()
                kw_result = {kw: " ".join(str(p) for p in sorted(pages)) for kw, pages in kw_pages.items()}
            except Exception:
                kw_result = {kw: "" for kw in keywords}

            has_any_hit = any(v for v in kw_result.values())
            if has_any_hit:
                code = extract_code_from_pdf_filename(pdf_name)
                pdf_key = norm_key(pdf_name)
                meta = meta_index.get(pdf_key, {})
                row = {
                    "æ—¥ä»˜": d, "ã‚³ãƒ¼ãƒ‰": code,
                    "ä¼æ¥­å": meta.get("ä¼šç¤¾å", ""),
                    "åˆ†é¡": meta.get("åˆ†é¡", "ãã®ä»–"),
                    "PDF": meta.get("URL", ""),
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)
            if progress_callback:
                progress_callback(processed, total_pdfs)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ B/C: JSONçµŒç”±æ¤œç´¢
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_text_index_remote() -> list[str]:
    url = f"{GITHUB_PAGES_TEXT_BASE}/index.json"
    try:
        resp = _requests.get(url, timeout=10)
        resp.raise_for_status()
        return resp.json().get("dates", [])
    except Exception:
        return []


def list_text_json_dates_local(text_dir: str) -> list[str]:
    if not os.path.isdir(text_dir):
        return []
    return sorted([
        m.group(1) for fn in os.listdir(text_dir)
        if (m := re.match(r"text_(\d{8})\.json$", fn))
    ])


@st.cache_data(ttl=600, show_spinner=False)
def load_text_json_remote(date_str: str) -> dict:
    url = f"{GITHUB_PAGES_TEXT_BASE}/text_{date_str}.json"
    try:
        resp = _requests.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception:
        return {}


def load_text_json_local(text_dir: str, date_str: str) -> dict:
    path = os.path.join(text_dir, f"text_{date_str}.json")
    if not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def search_text_json(
    date_from: str, date_to: str, keywords: list[str],
    available_dates: list[str], load_func, progress_callback=None,
) -> pd.DataFrame:
    target_dates = [d for d in available_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    results = []
    total_dates = len(target_dates)

    for idx, d in enumerate(target_dates):
        data = load_func(d)
        if not data or "files" not in data:
            if progress_callback:
                progress_callback(idx + 1, total_dates)
            continue

        for file_info in data["files"]:
            pages = file_info.get("pages", [])
            kw_result = {}
            for kw in keywords:
                hit_pages = []
                for page_idx, page_text in enumerate(pages, start=1):
                    if kw in page_text:
                        hit_pages.append(str(page_idx))
                kw_result[kw] = " ".join(hit_pages)

            if any(v for v in kw_result.values()):
                row = {
                    "æ—¥ä»˜": d,
                    "ã‚³ãƒ¼ãƒ‰": file_info.get("code", ""),
                    "ä¼æ¥­å": file_info.get("company", ""),
                    "åˆ†é¡": file_info.get("category", "ãã®ä»–"),
                    "PDF": file_info.get("url", ""),
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)

        if progress_callback:
            progress_callback(idx + 1, total_dates)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# Streamlit UI
# ============================================================
def main():
    st.set_page_config(page_title="TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", page_icon="ğŸ”", layout="wide")

    st.title("TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    st.caption("TDneté©æ™‚é–‹ç¤ºPDFã‹ã‚‰ã€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

    # ----- ã‚µã‚¤ãƒ‰ãƒãƒ¼ -----
    with st.sidebar:
        st.header("æ¤œç´¢æ¡ä»¶")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            options=[
                "ãƒ­ãƒ¼ã‚«ãƒ«PDFï¼ˆç›´æ¥æ¤œç´¢ï¼‰",
                "ãƒ­ãƒ¼ã‚«ãƒ«JSONï¼ˆé«˜é€Ÿæ¤œç´¢ï¼‰",
                "ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰",
            ],
            index=0,
            help=(
                "ãƒ­ãƒ¼ã‚«ãƒ«PDF: PCã®PDFã‚’ç›´æ¥æ¤œç´¢ï¼ˆé…ã„ãŒç¢ºå®Ÿï¼‰\n"
                "ãƒ­ãƒ¼ã‚«ãƒ«JSON: â‘¥ã§äº‹å‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã§é«˜é€Ÿæ¤œç´¢\n"
                "ã‚¯ãƒ©ã‚¦ãƒ‰: GitHub Pagesã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œç´¢ï¼ˆPDFä¸è¦ï¼‰"
            ),
        )

        is_local_pdf = "ãƒ­ãƒ¼ã‚«ãƒ«PDF" in data_source
        is_local_json = "ãƒ­ãƒ¼ã‚«ãƒ«JSON" in data_source
        is_cloud = "ã‚¯ãƒ©ã‚¦ãƒ‰" in data_source

        pdf_root = ""
        text_json_dir = DEFAULT_TEXT_JSON_DIR

        if is_local_pdf:
            pdf_root = st.text_input(
                "PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹", value=DEFAULT_PDF_ROOT,
                help="â‘ ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€",
            )
            available_dates = list_date_folders(pdf_root)
            if not available_dates:
                st.warning(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_root}")
                st.stop()

        elif is_local_json:
            text_json_dir = st.text_input(
                "ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ•ã‚©ãƒ«ãƒ€", value=DEFAULT_TEXT_JSON_DIR,
                help="â‘¥ã§æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆJSONã®ãƒ•ã‚©ãƒ«ãƒ€",
            )
            available_dates = list_text_json_dates_local(text_json_dir)
            if not available_dates:
                st.warning(
                    f"ãƒ†ã‚­ã‚¹ãƒˆJSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {text_json_dir}\n\n"
                    "â‘¥_pdf_text_extractor.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )
                st.stop()

        else:  # is_cloud
            with st.spinner("åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèªä¸­..."):
                available_dates = fetch_text_index_remote()
            if not available_dates:
                st.warning(
                    "ã‚¯ãƒ©ã‚¦ãƒ‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
                    "GitHub Actions ã®æ‰‹å‹•å®Ÿè¡ŒãŒå¿…è¦ã§ã™:\n"
                    "1. GitHub â†’ tdnet_get â†’ Actions\n"
                    "2. 'Daily XBRL Update' â†’ Run workflow"
                )
                st.stop()

        st.info(f"åˆ©ç”¨å¯èƒ½: {available_dates[0]} ã€œ {available_dates[-1]}ï¼ˆ{len(available_dates)}æ—¥åˆ†ï¼‰")

        # æœŸé–“æŒ‡å®š
        min_date = datetime.datetime.strptime(available_dates[0], "%Y%m%d").date()
        max_date = datetime.datetime.strptime(available_dates[-1], "%Y%m%d").date()
        col1, col2 = st.columns(2)
        with col1:
            date_from = st.date_input("é–‹å§‹æ—¥", value=max_date, min_value=min_date, max_value=max_date)
        with col2:
            date_to = st.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date)

        st.divider()

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
        st.subheader("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å€‹ï¼‰")
        keywords_input = []
        default_keywords = ["å¢—ç”£", "ä¸Šæ–¹ä¿®æ­£", "ã‚·ã‚§ã‚¢æ‹¡å¤§", "ä¾¡æ ¼æ”¹å®š", "éœ€è¦å›å¾©"]
        for i in range(5):
            kw = st.text_input(
                f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}",
                value=default_keywords[i] if i < len(default_keywords) else "",
                key=f"kw_{i}",
                label_visibility="collapsed" if i > 0 else "visible",
                placeholder=f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}ï¼ˆç©ºæ¬„ã¯ç„¡è¦–ï¼‰",
            )
            if kw.strip():
                keywords_input.append(kw.strip())

        st.divider()
        search_clicked = st.button("æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True)

        if keywords_input:
            st.caption(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_input)}")
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # ----- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ -----
    if search_clicked and keywords_input:
        d_from = date_from.strftime("%Y%m%d")
        d_to = date_to.strftime("%Y%m%d")

        if d_from > d_to:
            st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        st.subheader(f"æ¤œç´¢çµæœ: {d_from} ã€œ {d_to}")
        progress_bar = st.progress(0, text="æ¤œç´¢ä¸­...")

        if is_local_pdf:
            if fitz is None:
                st.error("PyMuPDF ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install pymupdf`")
                st.stop()

            def update_progress(current, total):
                progress_bar.progress(current / total if total else 0, text=f"PDFæ¤œç´¢ä¸­... ({current}/{total})")

            df = search_pdfs_local(pdf_root, d_from, d_to, keywords_input, progress_callback=update_progress)

        elif is_local_json:
            def update_progress(current, total):
                progress_bar.progress(current / total if total else 0, text=f"ãƒ†ã‚­ã‚¹ãƒˆæ¤œç´¢ä¸­... ({current}/{total}æ—¥)")

            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=lambda d: load_text_json_local(text_json_dir, d),
                progress_callback=update_progress,
            )
        else:
            def update_progress(current, total):
                progress_bar.progress(current / total if total else 0, text=f"ã‚¯ãƒ©ã‚¦ãƒ‰èª­ã¿è¾¼ã¿ä¸­... ({current}/{total}æ—¥)")

            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=load_text_json_remote,
                progress_callback=update_progress,
            )

        progress_bar.empty()

        st.session_state["search_results"] = df
        st.session_state["search_keywords"] = keywords_input

    # ----- çµæœè¡¨ç¤º -----
    df = st.session_state.get("search_results")
    keywords_display = st.session_state.get("search_keywords", [])

    if df is not None:
        if df.empty:
            st.info("ãƒ’ãƒƒãƒˆã™ã‚‹PDFã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            # åˆ†é¡ãƒ•ã‚£ãƒ«ã‚¿
            all_categories = sorted(df["åˆ†é¡"].unique().tolist())
            selected_categories = st.multiselect(
                "åˆ†é¡ã§ãƒ•ã‚£ãƒ«ã‚¿", options=all_categories, default=all_categories,
            )
            filtered_df = df[df["åˆ†é¡"].isin(selected_categories)] if selected_categories else df

            st.metric("ãƒ’ãƒƒãƒˆæ•°", f"{len(filtered_df)} ä»¶ / å…¨ {len(df)} ä»¶")

            # è¡¨ç¤ºç”¨DataFrame
            display_df = filtered_df.copy().reset_index(drop=True)
            display_df["æ—¥ä»˜"] = display_df["æ—¥ä»˜"].apply(
                lambda x: f"{x[:4]}/{x[4:6]}/{x[6:]}" if len(str(x)) == 8 else x
            )

            # ã‚«ãƒ©ãƒ é †: æ—¥ä»˜, ã‚³ãƒ¼ãƒ‰, ä¼æ¥­å, åˆ†é¡, PDFï¼ˆTDnetãƒªãƒ³ã‚¯ï¼‰, ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ—
            display_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡", "PDF"] + keywords_display
            display_df = display_df[[c for c in display_cols if c in display_df.columns]]

            # ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤ºï¼ˆãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ãªã—ã€PDFã¯ã‚¯ãƒªãƒƒã‚¯å¯èƒ½ãƒªãƒ³ã‚¯ï¼‰
            st.dataframe(
                display_df,
                use_container_width=True,
                hide_index=True,
                height=min(len(display_df) * 40 + 40, 600),
                column_config={
                    "PDF": st.column_config.LinkColumn(
                        "PDF",
                        display_text="é–‹ã",
                        help="ã‚¯ãƒªãƒƒã‚¯ã§TDnetã®PDFã‚’è¡¨ç¤º",
                    ),
                },
            )

            st.caption("â€» TDnetã®PDFãƒªãƒ³ã‚¯ã¯å…¬é–‹ã‹ã‚‰ç´„30æ—¥ã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv_data = filtered_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_data,
                file_name=f"keyword_search_{d_from}_{d_to}.csv", mime="text/csv",
            )

    elif df is None:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æœŸé–“ã‚’è¨­å®šã—ã€ã€Œæ¤œç´¢é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
