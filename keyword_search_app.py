# -*- coding: utf-8 -*-
"""
TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ Webã‚¢ãƒ—ãƒª (Streamlit)

èµ·å‹•æ–¹æ³•:
  ãƒ­ãƒ¼ã‚«ãƒ«:  streamlit run keyword_search_app.py
  ã‚¯ãƒ©ã‚¦ãƒ‰:  Streamlit Cloud ã«ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆPDFä¸è¦ã€JSONçµŒç”±ã§æ¤œç´¢ï¼‰

å‹•ä½œãƒ¢ãƒ¼ãƒ‰:
  A) ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿ -- PyMuPDFã§PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå€‹äººç”¨ï¼‰
  B) JSONçµŒç”±æ¤œç´¢     -- äº‹å‰æŠ½å‡ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆJSONã§æ¤œç´¢ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ã€PDFä¸è¦ï¼‰
"""

import os
import re
import json
import datetime
import unicodedata
import pandas as pd
import streamlit as st

try:
    import requests as _requests
except ImportError:
    _requests = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

# ============================================================
# è¨­å®š
# ============================================================
DEFAULT_PDF_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"
DEFAULT_TEXT_JSON_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "text_data")

# GitHub Pages ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ™ãƒ¼ã‚¹URL
GITHUB_PAGES_TEXT_BASE = "https://onokazu777.github.io/tdnet-viewer/data/text"

PRIORITY_KEYWORDS = ["äº‹æ¥­è¨ˆç”»", "äºˆæƒ³ã®ä¿®æ­£", "æ±ºç®—çŸ­ä¿¡", "èª¬æ˜è³‡æ–™", "æœˆæ¬¡", "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡"]


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def norm_key(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()


def get_category(title: str) -> str:
    for kw in PRIORITY_KEYWORDS:
        if kw in title:
            return kw
    return "ãã®ä»–"


def extract_code_from_pdf_filename(pdf_filename: str) -> str:
    m = re.match(r"^([0-9A-Za-z]{4})_", str(pdf_filename))
    return m.group(1).upper() if m else ""


def list_date_folders(root_path: str) -> list[str]:
    if not os.path.isdir(root_path):
        return []
    return sorted([
        d for d in os.listdir(root_path)
        if os.path.isdir(os.path.join(root_path, d)) and re.fullmatch(r"\d{8}", d)
    ])


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿
# ============================================================
def load_tdnet_meta(root_path: str, date_str: str) -> dict:
    day_csv = os.path.join(root_path, date_str, f"TDnet_Sorted_{date_str}.csv")
    root_csv = os.path.join(root_path, f"TDnet_Sorted_{date_str}.csv")
    csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None
    if csv_path is None:
        return {}

    df = pd.read_csv(csv_path, dtype=str).fillna("")
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    if "PDFãƒ•ã‚¡ã‚¤ãƒ«å" not in df.columns:
        return {}

    index = {}
    for _, r in df.iterrows():
        pdf_key = norm_key(r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", ""))
        if not pdf_key:
            continue
        title_link = str(r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")).strip()
        display_text = str(r.get("ä¼šç¤¾å", "")).strip()
        m = re.match(r'=HYPERLINK\("([^"]*)",\s*"([^"]*)"\)', title_link)
        url = ""
        if m:
            url = m.group(1)
            display_text = m.group(2) or display_text
        bunrui = str(r.get("åˆ†é¡", "")).strip()
        if not bunrui:
            bunrui = get_category(display_text)
        index[pdf_key] = {
            "ä¼šç¤¾å": str(r.get("ä¼šç¤¾å", "")).strip(),
            "ã‚³ãƒ¼ãƒ‰": str(r.get("ã‚³ãƒ¼ãƒ‰", "")).strip()[:4],
            "åˆ†é¡": bunrui,
            "è¡¨é¡Œ": display_text,
            "URL": url or str(r.get("URLï¼ˆç”Ÿï¼‰", "")).strip(),
        }
    return index


def search_pdfs_local(
    root_path: str, date_from: str, date_to: str, keywords: list[str], progress_callback=None,
) -> pd.DataFrame:
    """ãƒ­ãƒ¼ã‚«ãƒ«PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
    all_dates = list_date_folders(root_path)
    target_dates = [d for d in all_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    total_pdfs = 0
    date_pdfs: dict[str, list[str]] = {}
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        pdfs = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]
        date_pdfs[d] = pdfs
        total_pdfs += len(pdfs)
    if total_pdfs == 0:
        return pd.DataFrame()

    results = []
    processed = 0
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        meta_index = load_tdnet_meta(root_path, d)
        for pdf_name in sorted(date_pdfs[d]):
            processed += 1
            pdf_path = os.path.join(day_dir, pdf_name)

            try:
                doc = fitz.open(pdf_path)
                kw_pages = {kw: set() for kw in keywords}
                for page_index, page in enumerate(doc, start=1):
                    text = page.get_text("text")
                    for kw in keywords:
                        if kw in text:
                            kw_pages[kw].add(page_index)
                doc.close()
                kw_result = {kw: " ".join(str(p) for p in sorted(pages)) for kw, pages in kw_pages.items()}
            except Exception:
                kw_result = {kw: "" for kw in keywords}

            has_any_hit = any(v for v in kw_result.values())
            if has_any_hit:
                code = extract_code_from_pdf_filename(pdf_name)
                pdf_key = norm_key(pdf_name)
                meta = meta_index.get(pdf_key, {})
                local_pdf_path = os.path.join(root_path, d, pdf_name)
                row = {
                    "æ—¥ä»˜": d, "ã‚³ãƒ¼ãƒ‰": code,
                    "ä¼æ¥­å": meta.get("ä¼šç¤¾å", ""), "åˆ†é¡": meta.get("åˆ†é¡", "ãã®ä»–"),
                    "TDnet_URL": meta.get("URL", ""), "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": local_pdf_path,
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)
            if progress_callback:
                progress_callback(processed, total_pdfs)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: JSONçµŒç”±æ¤œç´¢ï¼ˆã‚¯ãƒ©ã‚¦ãƒ‰å¯¾å¿œï¼‰
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_text_index_remote() -> list[str]:
    """GitHub Pages ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆJSONä¸€è¦§ã‚’å–å¾—"""
    url = f"{GITHUB_PAGES_TEXT_BASE}/index.json"
    try:
        resp = _requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        return data.get("dates", [])
    except Exception:
        return []


def list_text_json_dates_local(text_dir: str) -> list[str]:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ä¸€è¦§ã‚’å–å¾—"""
    if not os.path.isdir(text_dir):
        return []
    dates = []
    for fn in os.listdir(text_dir):
        m = re.match(r"text_(\d{8})\.json$", fn)
        if m:
            dates.append(m.group(1))
    return sorted(dates)


@st.cache_data(ttl=600, show_spinner=False)
def load_text_json_remote(date_str: str) -> dict:
    """GitHub Pages ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆJSONã‚’å–å¾—"""
    url = f"{GITHUB_PAGES_TEXT_BASE}/text_{date_str}.json"
    try:
        resp = _requests.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception:
        return {}


def load_text_json_local(text_dir: str, date_str: str) -> dict:
    """ãƒ­ãƒ¼ã‚«ãƒ«ã®ãƒ†ã‚­ã‚¹ãƒˆJSONã‚’èª­ã¿è¾¼ã‚€"""
    path = os.path.join(text_dir, f"text_{date_str}.json")
    if not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def search_text_json(
    date_from: str, date_to: str, keywords: list[str],
    available_dates: list[str], load_func, progress_callback=None,
) -> pd.DataFrame:
    """äº‹å‰æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆJSONã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
    target_dates = [d for d in available_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    results = []
    total_dates = len(target_dates)

    for idx, d in enumerate(target_dates):
        data = load_func(d)
        if not data or "files" not in data:
            if progress_callback:
                progress_callback(idx + 1, total_dates)
            continue

        for file_info in data["files"]:
            pages = file_info.get("pages", [])
            kw_result = {}

            for kw in keywords:
                hit_pages = []
                for page_idx, page_text in enumerate(pages, start=1):
                    if kw in page_text:
                        hit_pages.append(str(page_idx))
                kw_result[kw] = " ".join(hit_pages)

            has_any_hit = any(v for v in kw_result.values())
            if has_any_hit:
                row = {
                    "æ—¥ä»˜": d,
                    "ã‚³ãƒ¼ãƒ‰": file_info.get("code", ""),
                    "ä¼æ¥­å": file_info.get("company", ""),
                    "åˆ†é¡": file_info.get("category", "ãã®ä»–"),
                    "TDnet_URL": file_info.get("url", ""),
                    "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": "",
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)

        if progress_callback:
            progress_callback(idx + 1, total_dates)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# Streamlit UI
# ============================================================
def main():
    st.set_page_config(page_title="TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", page_icon="ğŸ”", layout="wide")

    st.title("TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    st.caption("TDneté©æ™‚é–‹ç¤ºPDFã‹ã‚‰ã€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

    # ----- ã‚µã‚¤ãƒ‰ãƒãƒ¼ -----
    with st.sidebar:
        st.header("æ¤œç´¢æ¡ä»¶")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ‡ã‚Šæ›¿ãˆ
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            options=["ãƒ­ãƒ¼ã‚«ãƒ«PDFï¼ˆå€‹äººç”¨ï¼‰", "ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰"],
            index=0,
            help="ãƒ­ãƒ¼ã‚«ãƒ«: PCã®PDFã‚’ç›´æ¥æ¤œç´¢ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰: GitHub Pagesã®äº‹å‰æŠ½å‡ºãƒ‡ãƒ¼ã‚¿ã§æ¤œç´¢ï¼ˆPDFä¸è¦ï¼‰ã€‚",
        )
        is_cloud = "ã‚¯ãƒ©ã‚¦ãƒ‰" in data_source

        if not is_cloud:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            pdf_root = st.text_input(
                "PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹", value=DEFAULT_PDF_ROOT,
                help="â‘ ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€",
            )
            available_dates = list_date_folders(pdf_root)
            if not available_dates:
                st.warning(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_root}")
                st.stop()

            # ãƒ†ã‚­ã‚¹ãƒˆJSONã‚‚ãƒ­ãƒ¼ã‚«ãƒ«ã«ã‚ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
            text_json_dir = DEFAULT_TEXT_JSON_DIR
            text_dates = list_text_json_dates_local(text_json_dir)
        else:
            # ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ¼ãƒ‰è¨­å®š
            pdf_root = ""
            with st.spinner("åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèªä¸­..."):
                available_dates = fetch_text_index_remote()
            if not available_dates:
                st.warning("ã‚¯ãƒ©ã‚¦ãƒ‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ã¾ã GitHub ActionsãŒå®Ÿè¡Œã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                st.stop()
            text_dates = available_dates

        st.info(f"åˆ©ç”¨å¯èƒ½: {available_dates[0]} ã€œ {available_dates[-1]}ï¼ˆ{len(available_dates)}æ—¥åˆ†ï¼‰")

        # æœŸé–“æŒ‡å®š
        min_date = datetime.datetime.strptime(available_dates[0], "%Y%m%d").date()
        max_date = datetime.datetime.strptime(available_dates[-1], "%Y%m%d").date()
        col1, col2 = st.columns(2)
        with col1:
            date_from = st.date_input("é–‹å§‹æ—¥", value=max_date, min_value=min_date, max_value=max_date)
        with col2:
            date_to = st.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date)

        st.divider()

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
        st.subheader("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å€‹ï¼‰")
        keywords_input = []
        default_keywords = ["å¢—ç”£", "ä¸Šæ–¹ä¿®æ­£", "ã‚·ã‚§ã‚¢æ‹¡å¤§", "ä¾¡æ ¼æ”¹å®š", "éœ€è¦å›å¾©"]
        for i in range(5):
            kw = st.text_input(
                f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}",
                value=default_keywords[i] if i < len(default_keywords) else "",
                key=f"kw_{i}",
                label_visibility="collapsed" if i > 0 else "visible",
                placeholder=f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}ï¼ˆç©ºæ¬„ã¯ç„¡è¦–ï¼‰",
            )
            if kw.strip():
                keywords_input.append(kw.strip())

        st.divider()

        # ãƒªãƒ³ã‚¯å…ˆï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰æ™‚ã®ã¿é¸æŠå¯èƒ½ï¼‰
        if not is_cloud:
            link_mode = st.radio(
                "PDFãƒªãƒ³ã‚¯å…ˆ",
                options=["TDnetï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰", "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆå€‹äººç”¨ï¼‰"],
                index=0,
            )
        else:
            link_mode = "TDnetï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰"
            st.caption("ãƒªãƒ³ã‚¯å…ˆ: TDnet")

        st.divider()
        search_clicked = st.button("æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True)

        if keywords_input:
            st.caption(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_input)}")
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # ----- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ -----
    if search_clicked and keywords_input:
        d_from = date_from.strftime("%Y%m%d")
        d_to = date_to.strftime("%Y%m%d")

        if d_from > d_to:
            st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        st.subheader(f"æ¤œç´¢çµæœ: {d_from} ã€œ {d_to}")
        progress_bar = st.progress(0, text="æ¤œç´¢ä¸­...")

        # æ¤œç´¢å®Ÿè¡Œ
        if is_cloud:
            # ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ¢ãƒ¼ãƒ‰: GitHub Pagesã®JSON
            def update_progress(current, total):
                pct = current / total if total > 0 else 0
                progress_bar.progress(pct, text=f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­... ({current}/{total}æ—¥)")

            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=load_text_json_remote,
                progress_callback=update_progress,
            )
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰: ãƒ†ã‚­ã‚¹ãƒˆJSONãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨ã€ãªã‘ã‚Œã°PDFç›´èª­ã¿
            local_target_dates = [d for d in available_dates if d_from <= d <= d_to]
            local_text_dates = [d for d in local_target_dates if d in text_dates]

            if len(local_text_dates) == len(local_target_dates) and local_text_dates:
                # ãƒ†ã‚­ã‚¹ãƒˆJSONãŒå…¨æ—¥ä»˜åˆ†ã‚ã‚‹ â†’ é«˜é€ŸJSONæ¤œç´¢
                def update_progress(current, total):
                    pct = current / total if total > 0 else 0
                    progress_bar.progress(pct, text=f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¤œç´¢ä¸­... ({current}/{total}æ—¥)")

                df = search_text_json(
                    d_from, d_to, keywords_input, text_dates,
                    load_func=lambda d: load_text_json_local(DEFAULT_TEXT_JSON_DIR, d),
                    progress_callback=update_progress,
                )
            else:
                # PDFç›´èª­ã¿
                if fitz is None:
                    st.error("PyMuPDF (fitz) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install pymupdf` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                    st.stop()

                def update_progress(current, total):
                    pct = current / total if total > 0 else 0
                    progress_bar.progress(pct, text=f"PDFæ¤œç´¢ä¸­... ({current}/{total})")

                df = search_pdfs_local(
                    pdf_root, d_from, d_to, keywords_input,
                    progress_callback=update_progress,
                )

        progress_bar.empty()

        if df.empty:
            st.info("ãƒ’ãƒƒãƒˆã™ã‚‹PDFã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            use_tdnet_link = "TDnet" in link_mode

            # åˆ†é¡ãƒ•ã‚£ãƒ«ã‚¿
            all_categories = sorted(df["åˆ†é¡"].unique().tolist())
            selected_categories = st.multiselect(
                "åˆ†é¡ã§ãƒ•ã‚£ãƒ«ã‚¿", options=all_categories, default=all_categories,
            )
            filtered_df = df[df["åˆ†é¡"].isin(selected_categories)] if selected_categories else df

            st.metric("ãƒ’ãƒƒãƒˆæ•°", f"{len(filtered_df)} ä»¶ / å…¨ {len(df)} ä»¶")

            # è¡¨ç¤ºç”¨DataFrame
            display_df = filtered_df.copy()
            display_df["æ—¥ä»˜"] = display_df["æ—¥ä»˜"].apply(
                lambda x: f"{x[:4]}/{x[4:6]}/{x[6:]}" if len(str(x)) == 8 else x
            )

            # PDFãƒªãƒ³ã‚¯åˆ—
            if use_tdnet_link:
                display_df["PDF"] = display_df["TDnet_URL"]
            else:
                display_df["PDF"] = display_df["ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹"].apply(
                    lambda p: f"file:///{p.replace(os.sep, '/')}" if p else ""
                )

            display_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡", "PDF"] + keywords_input
            display_df = display_df[[c for c in display_cols if c in display_df.columns]]

            st.dataframe(
                display_df, use_container_width=True, hide_index=True,
                height=min(len(display_df) * 40 + 40, 600),
                column_config={
                    "PDF": st.column_config.LinkColumn("PDF", display_text="é–‹ã"),
                },
            )

            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv_data = filtered_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_data,
                file_name=f"keyword_search_{d_from}_{d_to}.csv", mime="text/csv",
            )

            if use_tdnet_link:
                st.caption("â€» TDnetã®PDFãƒªãƒ³ã‚¯ã¯å…¬é–‹ã‹ã‚‰ç´„30æ—¥ã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

    elif not search_clicked:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æœŸé–“ã‚’è¨­å®šã—ã€ã€Œæ¤œç´¢é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
