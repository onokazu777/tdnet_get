# -*- coding: utf-8 -*-
"""
TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ Webã‚¢ãƒ—ãƒª (Streamlit)

èµ·å‹•æ–¹æ³•:
  ãƒ­ãƒ¼ã‚«ãƒ«:  streamlit run keyword_search_app.py
  ã‚¯ãƒ©ã‚¦ãƒ‰:  Streamlit Cloud ã«ãƒ‡ãƒ—ãƒ­ã‚¤ï¼ˆPDFä¸è¦ã€JSONçµŒç”±ã§æ¤œç´¢ï¼‰

å‹•ä½œãƒ¢ãƒ¼ãƒ‰:
  A) ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿ -- PyMuPDFã§PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå€‹äººç”¨ï¼‰
  B) ãƒ­ãƒ¼ã‚«ãƒ«JSONæ¤œç´¢  -- â‘¥ã§äº‹å‰æŠ½å‡ºæ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆJSONã§é«˜é€Ÿæ¤œç´¢ï¼ˆå€‹äººç”¨ï¼‰
  C) ã‚¯ãƒ©ã‚¦ãƒ‰JSONæ¤œç´¢   -- GitHub Pagesã®ãƒ†ã‚­ã‚¹ãƒˆJSONã§æ¤œç´¢ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ã€PDFä¸è¦ï¼‰
"""

import os
import re
import json
import sys
import platform
import subprocess
import datetime
import unicodedata
import pandas as pd
import streamlit as st

try:
    import requests as _requests
except ImportError:
    _requests = None

try:
    import fitz  # PyMuPDF
except ImportError:
    fitz = None

# ============================================================
# è¨­å®š
# ============================================================
DEFAULT_PDF_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"
DEFAULT_TEXT_JSON_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "text_data")

# GitHub Pages ä¸Šã®ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ™ãƒ¼ã‚¹URL
GITHUB_PAGES_TEXT_BASE = "https://onokazu777.github.io/tdnet-viewer/data/text"

PRIORITY_KEYWORDS = ["äº‹æ¥­è¨ˆç”»", "äºˆæƒ³ã®ä¿®æ­£", "æ±ºç®—çŸ­ä¿¡", "èª¬æ˜è³‡æ–™", "æœˆæ¬¡", "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡"]


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# ============================================================
def norm_key(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()


def get_category(title: str) -> str:
    for kw in PRIORITY_KEYWORDS:
        if kw in title:
            return kw
    return "ãã®ä»–"


def extract_code_from_pdf_filename(pdf_filename: str) -> str:
    m = re.match(r"^([0-9A-Za-z]{4})_", str(pdf_filename))
    return m.group(1).upper() if m else ""


def list_date_folders(root_path: str) -> list[str]:
    if not os.path.isdir(root_path):
        return []
    return sorted([
        d for d in os.listdir(root_path)
        if os.path.isdir(os.path.join(root_path, d)) and re.fullmatch(r"\d{8}", d)
    ])


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ A: ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿
# ============================================================
def load_tdnet_meta(root_path: str, date_str: str) -> dict:
    day_csv = os.path.join(root_path, date_str, f"TDnet_Sorted_{date_str}.csv")
    root_csv = os.path.join(root_path, f"TDnet_Sorted_{date_str}.csv")
    csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None
    if csv_path is None:
        return {}

    df = pd.read_csv(csv_path, dtype=str).fillna("")
    df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]
    if "PDFãƒ•ã‚¡ã‚¤ãƒ«å" not in df.columns:
        return {}

    index = {}
    for _, r in df.iterrows():
        pdf_key = norm_key(r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", ""))
        if not pdf_key:
            continue
        title_link = str(r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")).strip()
        display_text = str(r.get("ä¼šç¤¾å", "")).strip()
        m = re.match(r'=HYPERLINK\("([^"]*)",\s*"([^"]*)"\)', title_link)
        url = ""
        if m:
            url = m.group(1)
            display_text = m.group(2) or display_text
        bunrui = str(r.get("åˆ†é¡", "")).strip()
        if not bunrui:
            bunrui = get_category(display_text)
        index[pdf_key] = {
            "ä¼šç¤¾å": str(r.get("ä¼šç¤¾å", "")).strip(),
            "ã‚³ãƒ¼ãƒ‰": str(r.get("ã‚³ãƒ¼ãƒ‰", "")).strip()[:4],
            "åˆ†é¡": bunrui,
            "è¡¨é¡Œ": display_text,
            "URL": url or str(r.get("URLï¼ˆç”Ÿï¼‰", "")).strip(),
        }
    return index


def search_pdfs_local(
    root_path: str, date_from: str, date_to: str, keywords: list[str], progress_callback=None,
) -> pd.DataFrame:
    """ãƒ­ãƒ¼ã‚«ãƒ«PDFã‚’ç›´æ¥ã‚¹ã‚­ãƒ£ãƒ³ã—ã¦ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢"""
    all_dates = list_date_folders(root_path)
    target_dates = [d for d in all_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    total_pdfs = 0
    date_pdfs: dict[str, list[str]] = {}
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        pdfs = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]
        date_pdfs[d] = pdfs
        total_pdfs += len(pdfs)
    if total_pdfs == 0:
        return pd.DataFrame()

    results = []
    processed = 0
    for d in target_dates:
        day_dir = os.path.join(root_path, d)
        meta_index = load_tdnet_meta(root_path, d)
        for pdf_name in sorted(date_pdfs[d]):
            processed += 1
            pdf_path = os.path.join(day_dir, pdf_name)

            try:
                doc = fitz.open(pdf_path)
                kw_pages = {kw: set() for kw in keywords}
                for page_index, page in enumerate(doc, start=1):
                    text = page.get_text("text")
                    for kw in keywords:
                        if kw in text:
                            kw_pages[kw].add(page_index)
                doc.close()
                kw_result = {kw: " ".join(str(p) for p in sorted(pages)) for kw, pages in kw_pages.items()}
            except Exception:
                kw_result = {kw: "" for kw in keywords}

            has_any_hit = any(v for v in kw_result.values())
            if has_any_hit:
                code = extract_code_from_pdf_filename(pdf_name)
                pdf_key = norm_key(pdf_name)
                meta = meta_index.get(pdf_key, {})
                local_pdf_path = os.path.join(root_path, d, pdf_name)
                row = {
                    "æ—¥ä»˜": d, "ã‚³ãƒ¼ãƒ‰": code,
                    "ä¼æ¥­å": meta.get("ä¼šç¤¾å", ""), "åˆ†é¡": meta.get("åˆ†é¡", "ãã®ä»–"),
                    "TDnet_URL": meta.get("URL", ""), "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": local_pdf_path,
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)
            if progress_callback:
                progress_callback(processed, total_pdfs)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ B/C: JSONçµŒç”±æ¤œç´¢ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«JSON / ã‚¯ãƒ©ã‚¦ãƒ‰JSONï¼‰
# ============================================================
@st.cache_data(ttl=3600, show_spinner=False)
def fetch_text_index_remote() -> list[str]:
    """GitHub Pages ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆJSONä¸€è¦§ã‚’å–å¾—"""
    url = f"{GITHUB_PAGES_TEXT_BASE}/index.json"
    try:
        resp = _requests.get(url, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        return data.get("dates", [])
    except Exception:
        return []


def list_text_json_dates_local(text_dir: str) -> list[str]:
    if not os.path.isdir(text_dir):
        return []
    dates = []
    for fn in os.listdir(text_dir):
        m = re.match(r"text_(\d{8})\.json$", fn)
        if m:
            dates.append(m.group(1))
    return sorted(dates)


@st.cache_data(ttl=600, show_spinner=False)
def load_text_json_remote(date_str: str) -> dict:
    url = f"{GITHUB_PAGES_TEXT_BASE}/text_{date_str}.json"
    try:
        resp = _requests.get(url, timeout=30)
        resp.raise_for_status()
        return resp.json()
    except Exception:
        return {}


def load_text_json_local(text_dir: str, date_str: str) -> dict:
    path = os.path.join(text_dir, f"text_{date_str}.json")
    if not os.path.exists(path):
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def search_text_json(
    date_from: str, date_to: str, keywords: list[str],
    available_dates: list[str], load_func,
    pdf_root: str = "",
    progress_callback=None,
) -> pd.DataFrame:
    """äº‹å‰æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆJSONã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢

    pdf_root ãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°ã€ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã‚‚æ§‹ç¯‰ã™ã‚‹ã€‚
    """
    target_dates = [d for d in available_dates if date_from <= d <= date_to]
    if not target_dates:
        return pd.DataFrame()

    results = []
    total_dates = len(target_dates)

    for idx, d in enumerate(target_dates):
        data = load_func(d)
        if not data or "files" not in data:
            if progress_callback:
                progress_callback(idx + 1, total_dates)
            continue

        for file_info in data["files"]:
            pages = file_info.get("pages", [])
            kw_result = {}

            for kw in keywords:
                hit_pages = []
                for page_idx, page_text in enumerate(pages, start=1):
                    if kw in page_text:
                        hit_pages.append(str(page_idx))
                kw_result[kw] = " ".join(hit_pages)

            has_any_hit = any(v for v in kw_result.values())
            if has_any_hit:
                pdf_name = file_info.get("pdf", "")
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹ã®æ§‹ç¯‰ï¼ˆpdf_rootãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆï¼‰
                local_path = ""
                if pdf_root and pdf_name:
                    local_path = os.path.join(pdf_root, d, pdf_name)

                row = {
                    "æ—¥ä»˜": d,
                    "ã‚³ãƒ¼ãƒ‰": file_info.get("code", ""),
                    "ä¼æ¥­å": file_info.get("company", ""),
                    "åˆ†é¡": file_info.get("category", "ãã®ä»–"),
                    "TDnet_URL": file_info.get("url", ""),
                    "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹": local_path,
                }
                for kw in keywords:
                    row[kw] = kw_result.get(kw, "")
                results.append(row)

        if progress_callback:
            progress_callback(idx + 1, total_dates)

    return pd.DataFrame(results) if results else pd.DataFrame()


# ============================================================
# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã
# ============================================================
def _open_local_file(filepath: str):
    """OSã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ãƒ—ãƒªã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã"""
    try:
        if platform.system() == "Windows":
            os.startfile(filepath)
        elif platform.system() == "Darwin":  # macOS
            subprocess.Popen(["open", filepath])
        else:  # Linux
            subprocess.Popen(["xdg-open", filepath])
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“ã§ã—ãŸ: {e}")


# ============================================================
# Streamlit UI
# ============================================================
def main():
    st.set_page_config(page_title="TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢", page_icon="ğŸ”", layout="wide")

    st.title("TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢")
    st.caption("TDneté©æ™‚é–‹ç¤ºPDFã‹ã‚‰ã€æŒ‡å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸ã‚’æ¤œç´¢ã—ã¾ã™ã€‚")

    # ----- ã‚µã‚¤ãƒ‰ãƒãƒ¼ -----
    with st.sidebar:
        st.header("æ¤œç´¢æ¡ä»¶")

        # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ‡ã‚Šæ›¿ãˆï¼ˆ3ãƒ¢ãƒ¼ãƒ‰ï¼‰
        data_source = st.radio(
            "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹",
            options=[
                "ãƒ­ãƒ¼ã‚«ãƒ«PDFï¼ˆç›´æ¥æ¤œç´¢ï¼‰",
                "ãƒ­ãƒ¼ã‚«ãƒ«JSONï¼ˆé«˜é€Ÿæ¤œç´¢ï¼‰",
                "ã‚¯ãƒ©ã‚¦ãƒ‰ï¼ˆä¸€èˆ¬å…¬é–‹ç”¨ï¼‰",
            ],
            index=0,
            help=(
                "ãƒ­ãƒ¼ã‚«ãƒ«PDF: PCã®PDFã‚’ç›´æ¥æ¤œç´¢ï¼ˆé…ã„ãŒç¢ºå®Ÿï¼‰\n"
                "ãƒ­ãƒ¼ã‚«ãƒ«JSON: â‘¥ã§äº‹å‰æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆã§é«˜é€Ÿæ¤œç´¢\n"
                "ã‚¯ãƒ©ã‚¦ãƒ‰: GitHub Pagesã®ãƒ‡ãƒ¼ã‚¿ã§æ¤œç´¢ï¼ˆPDFä¸è¦ï¼‰"
            ),
        )

        is_local_pdf = "ãƒ­ãƒ¼ã‚«ãƒ«PDF" in data_source
        is_local_json = "ãƒ­ãƒ¼ã‚«ãƒ«JSON" in data_source
        is_cloud = "ã‚¯ãƒ©ã‚¦ãƒ‰" in data_source

        # --- å„ãƒ¢ãƒ¼ãƒ‰åˆ¥ã®è¨­å®š ---
        pdf_root = ""

        if is_local_pdf:
            pdf_root = st.text_input(
                "PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹", value=DEFAULT_PDF_ROOT,
                help="â‘ ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ•ã‚©ãƒ«ãƒ€",
            )
            available_dates = list_date_folders(pdf_root)
            if not available_dates:
                st.warning(f"PDFãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_root}")
                st.stop()

        elif is_local_json:
            pdf_root = st.text_input(
                "PDFãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹ï¼ˆãƒªãƒ³ã‚¯ç”¨ï¼‰", value=DEFAULT_PDF_ROOT,
                help="PDFãƒªãƒ³ã‚¯ã«ä½¿ç”¨ã™ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ãƒ‘ã‚¹",
            )
            text_json_dir = st.text_input(
                "ãƒ†ã‚­ã‚¹ãƒˆJSONãƒ•ã‚©ãƒ«ãƒ€", value=DEFAULT_TEXT_JSON_DIR,
                help="â‘¥ã§æŠ½å‡ºã—ãŸãƒ†ã‚­ã‚¹ãƒˆJSONã®ãƒ•ã‚©ãƒ«ãƒ€",
            )
            available_dates = list_text_json_dates_local(text_json_dir)
            if not available_dates:
                st.warning(
                    f"ãƒ†ã‚­ã‚¹ãƒˆJSONãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {text_json_dir}\n\n"
                    "â‘¥_pdf_text_extractor.py ã‚’å…ˆã«å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚"
                )
                st.stop()

        else:  # is_cloud
            with st.spinner("åˆ©ç”¨å¯èƒ½ãªæ—¥ä»˜ã‚’ç¢ºèªä¸­..."):
                available_dates = fetch_text_index_remote()
            if not available_dates:
                st.warning(
                    "ã‚¯ãƒ©ã‚¦ãƒ‰ã«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
                    "GitHub Actions ã®æ‰‹å‹•å®Ÿè¡ŒãŒå¿…è¦ã§ã™:\n"
                    "1. GitHub â†’ tdnet_get â†’ Actions\n"
                    "2. 'Daily XBRL Update' â†’ Run workflow"
                )
                st.stop()

        st.info(f"åˆ©ç”¨å¯èƒ½: {available_dates[0]} ã€œ {available_dates[-1]}ï¼ˆ{len(available_dates)}æ—¥åˆ†ï¼‰")

        # æœŸé–“æŒ‡å®š
        min_date = datetime.datetime.strptime(available_dates[0], "%Y%m%d").date()
        max_date = datetime.datetime.strptime(available_dates[-1], "%Y%m%d").date()
        col1, col2 = st.columns(2)
        with col1:
            date_from = st.date_input("é–‹å§‹æ—¥", value=max_date, min_value=min_date, max_value=max_date)
        with col2:
            date_to = st.date_input("çµ‚äº†æ—¥", value=max_date, min_value=min_date, max_value=max_date)

        st.divider()

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å…¥åŠ›
        st.subheader("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆæœ€å¤§5å€‹ï¼‰")
        keywords_input = []
        default_keywords = ["å¢—ç”£", "ä¸Šæ–¹ä¿®æ­£", "ã‚·ã‚§ã‚¢æ‹¡å¤§", "ä¾¡æ ¼æ”¹å®š", "éœ€è¦å›å¾©"]
        for i in range(5):
            kw = st.text_input(
                f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}",
                value=default_keywords[i] if i < len(default_keywords) else "",
                key=f"kw_{i}",
                label_visibility="collapsed" if i > 0 else "visible",
                placeholder=f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {i + 1}ï¼ˆç©ºæ¬„ã¯ç„¡è¦–ï¼‰",
            )
            if kw.strip():
                keywords_input.append(kw.strip())

        st.divider()

        # PDFãƒªãƒ³ã‚¯å…ˆ
        if is_cloud:
            link_mode = "TDnet"
            st.caption("ãƒªãƒ³ã‚¯å…ˆ: TDnetï¼ˆå…¬é–‹ãƒªãƒ³ã‚¯ï¼‰")
        else:
            link_mode = st.radio(
                "PDFãƒªãƒ³ã‚¯å…ˆ",
                options=["TDnet", "ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«"],
                index=1,  # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ¼ãƒ‰ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«
            )

        st.divider()
        search_clicked = st.button("æ¤œç´¢é–‹å§‹", type="primary", use_container_width=True)

        if keywords_input:
            st.caption(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {', '.join(keywords_input)}")
        else:
            st.warning("ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’1ã¤ä»¥ä¸Šå…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")

    # ----- ãƒ¡ã‚¤ãƒ³ã‚¨ãƒªã‚¢ -----
    if search_clicked and keywords_input:
        d_from = date_from.strftime("%Y%m%d")
        d_to = date_to.strftime("%Y%m%d")

        if d_from > d_to:
            st.error("é–‹å§‹æ—¥ã¯çµ‚äº†æ—¥ä»¥å‰ã«ã—ã¦ãã ã•ã„ã€‚")
            st.stop()

        st.subheader(f"æ¤œç´¢çµæœ: {d_from} ã€œ {d_to}")
        progress_bar = st.progress(0, text="æ¤œç´¢ä¸­...")

        # ========== æ¤œç´¢å®Ÿè¡Œ ==========
        if is_local_pdf:
            # ãƒ¢ãƒ¼ãƒ‰A: ãƒ­ãƒ¼ã‚«ãƒ«PDFç›´èª­ã¿
            if fitz is None:
                st.error("PyMuPDF ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚`pip install pymupdf` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                st.stop()

            def update_progress(current, total):
                pct = current / total if total > 0 else 0
                progress_bar.progress(pct, text=f"PDFæ¤œç´¢ä¸­... ({current}/{total})")

            df = search_pdfs_local(
                pdf_root, d_from, d_to, keywords_input,
                progress_callback=update_progress,
            )

        elif is_local_json:
            # ãƒ¢ãƒ¼ãƒ‰B: ãƒ­ãƒ¼ã‚«ãƒ«JSONé«˜é€Ÿæ¤œç´¢
            def update_progress(current, total):
                pct = current / total if total > 0 else 0
                progress_bar.progress(pct, text=f"ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ¤œç´¢ä¸­... ({current}/{total}æ—¥)")

            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=lambda d: load_text_json_local(text_json_dir, d),
                pdf_root=pdf_root,
                progress_callback=update_progress,
            )

        else:
            # ãƒ¢ãƒ¼ãƒ‰C: ã‚¯ãƒ©ã‚¦ãƒ‰JSONæ¤œç´¢
            def update_progress(current, total):
                pct = current / total if total > 0 else 0
                progress_bar.progress(pct, text=f"ã‚¯ãƒ©ã‚¦ãƒ‰ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ä¸­... ({current}/{total}æ—¥)")

            df = search_text_json(
                d_from, d_to, keywords_input, available_dates,
                load_func=load_text_json_remote,
                pdf_root="",
                progress_callback=update_progress,
            )

        progress_bar.empty()

        # æ¤œç´¢çµæœã‚’session_stateã«ä¿å­˜ï¼ˆè¡Œé¸æŠæ™‚ã®å†å®Ÿè¡Œã§æ¶ˆãˆãªã„ã‚ˆã†ã«ï¼‰
        st.session_state["search_results"] = df
        st.session_state["search_keywords"] = keywords_input
        st.session_state["search_link_mode"] = link_mode

    # ----- çµæœè¡¨ç¤ºï¼ˆsession_stateã‹ã‚‰ï¼‰ -----
    df = st.session_state.get("search_results")
    keywords_display = st.session_state.get("search_keywords", [])
    link_mode_display = st.session_state.get("search_link_mode", "TDnet")

    if df is not None:
        if df.empty:
            st.info("ãƒ’ãƒƒãƒˆã™ã‚‹PDFã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        else:
            use_tdnet_link = "TDnet" in link_mode_display

            # åˆ†é¡ãƒ•ã‚£ãƒ«ã‚¿
            all_categories = sorted(df["åˆ†é¡"].unique().tolist())
            selected_categories = st.multiselect(
                "åˆ†é¡ã§ãƒ•ã‚£ãƒ«ã‚¿", options=all_categories, default=all_categories,
            )
            filtered_df = df[df["åˆ†é¡"].isin(selected_categories)] if selected_categories else df

            st.metric("ãƒ’ãƒƒãƒˆæ•°", f"{len(filtered_df)} ä»¶ / å…¨ {len(df)} ä»¶")

            # è¡¨ç¤ºç”¨DataFrame
            display_df = filtered_df.copy().reset_index(drop=True)
            display_df["æ—¥ä»˜"] = display_df["æ—¥ä»˜"].apply(
                lambda x: f"{x[:4]}/{x[4:6]}/{x[6:]}" if len(str(x)) == 8 else x
            )

            # --- TDnetãƒªãƒ³ã‚¯ãƒ¢ãƒ¼ãƒ‰: ãƒªãƒ³ã‚¯ä»˜ããƒ†ãƒ¼ãƒ–ãƒ« ---
            if use_tdnet_link:
                display_df["PDF"] = display_df["TDnet_URL"].apply(
                    lambda u: u if u else ""
                )
                display_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡", "PDF"] + keywords_display
                display_df = display_df[[c for c in display_cols if c in display_df.columns]]

                st.dataframe(
                    display_df, use_container_width=True, hide_index=True,
                    height=min(len(display_df) * 40 + 40, 600),
                    column_config={
                        "PDF": st.column_config.LinkColumn(
                            "PDF", display_text="é–‹ã",
                            help="TDnetã®PDFãƒªãƒ³ã‚¯",
                        ),
                    },
                )
                st.caption("â€» TDnetã®PDFãƒªãƒ³ã‚¯ã¯å…¬é–‹ã‹ã‚‰ç´„30æ—¥ã§ç„¡åŠ¹ã«ãªã‚Šã¾ã™ã€‚")

            # --- ãƒ­ãƒ¼ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ¢ãƒ¼ãƒ‰: è¡Œé¸æŠã§PDFã‚’é–‹ã ---
            else:
                display_cols = ["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "ä¼æ¥­å", "åˆ†é¡"] + keywords_display
                table_df = display_df[[c for c in display_cols if c in display_df.columns]]

                event = st.dataframe(
                    table_df, use_container_width=True, hide_index=True,
                    height=min(len(table_df) * 40 + 40, 600),
                    on_select="rerun",
                    selection_mode="single-row",
                )

                # é¸æŠã•ã‚ŒãŸè¡Œã®PDFã‚’é–‹ã
                selected_rows = event.selection.rows if event.selection else []

                if selected_rows:
                    sel_idx = selected_rows[0]
                    sel_row = filtered_df.iloc[sel_idx]
                    pdf_path = sel_row.get("ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹", "")
                    company = sel_row.get("ä¼æ¥­å", "")
                    category = sel_row.get("åˆ†é¡", "")

                    st.markdown(f"**é¸æŠä¸­:** {company}ï¼ˆ{category}ï¼‰")

                    col_open, col_path = st.columns([1, 4])
                    with col_open:
                        if st.button("PDFã‚’é–‹ã", type="primary", use_container_width=True):
                            if pdf_path and os.path.exists(pdf_path):
                                _open_local_file(pdf_path)
                                st.success("PDFã‚’é–‹ãã¾ã—ãŸ")
                            elif pdf_path:
                                st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {pdf_path}")
                            else:
                                st.error("ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãŒã‚ã‚Šã¾ã›ã‚“")
                    with col_path:
                        st.code(pdf_path, language=None)
                else:
                    st.caption("ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨PDFã‚’é–‹ã‘ã¾ã™ã€‚")

            # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            csv_data = filtered_df.to_csv(index=False, encoding="utf-8-sig")
            st.download_button(
                label="çµæœã‚’CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", data=csv_data,
                file_name=f"keyword_search_{d_from}_{d_to}.csv", mime="text/csv",
            )

    elif df is None:
        st.info("å·¦ã®ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¨æœŸé–“ã‚’è¨­å®šã—ã€ã€Œæ¤œç´¢é–‹å§‹ã€ãƒœã‚¿ãƒ³ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")


if __name__ == "__main__":
    main()
