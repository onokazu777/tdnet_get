# -*- coding: utf-8 -*-
"""
â‘  TDnetã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ï¼ˆæœˆæ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰

ã€Windowsãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç‰ˆã€‘
- Colabå°‚ç”¨ã® `!pip` / `google.colab` / `/content/drive` ä¾å­˜ã‚’é™¤å»
- ä¿å­˜å…ˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«/ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ‘ã‚¹ã«å¯¾å¿œ

å®Ÿè¡Œä¾‹:
  python "â‘ _tdnetã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ï¼ˆæœˆæ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰.py" --target "20260202"
  python "â‘ _tdnetã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ï¼ˆæœˆæ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰.py" --target "20260105 20260109"
"""

# ============================================================
# â‘  TDnetã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å‡¦ç†ï¼ˆç¯„å›²æŒ‡å®šå¯¾å¿œç‰ˆï¼‰â€»ä¿®æ­£ç‰ˆï¼ˆæ­£è¦åŒ–å¼·åŒ–ï¼‰
#
# ã§ãã‚‹ã“ã¨
# - TDnetã®ä¸€è¦§ã‚’ã€Œæ—¥åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ï¼ˆYYYYMMDDï¼‰ã€ã«ä¿å­˜ã—ãªãŒã‚‰ã€è¤‡æ•°æ—¥ã‚’ã¾ã¨ã‚ã¦å›å
# - å¯¾è±¡æŒ‡å®šã¯ 3ãƒ‘ã‚¿ãƒ¼ãƒ³
#   A) æ—¥åˆ¥: "20260109"
#   B) æœˆæŒ‡å®š: "202601"           ï¼ˆ2026å¹´1æœˆï¼‰
#   C) ç¯„å›²:  "20260105 20260109" ï¼ˆfrom toï¼‰
#
# ä¿å­˜å…ˆï¼ˆGoogle Driveï¼‰
#   MyDrive/{SAVE_ROOT}/{YYYYMMDD}/
#     â”œâ”€â”€ *.pdf
#     â””â”€â”€ TDnet_Sorted_YYYYMMDD.csv
#   â€»æ—¥åˆ¥CSVã¯åŒæ™‚ã«ãƒ«ãƒ¼ãƒˆç›´ä¸‹ã«ã‚‚ã‚³ãƒ”ãƒ¼ä¿å­˜ã™ã‚‹
#
# ä¿®æ­£ç‰ˆã®ä¸»ç›®çš„ï¼ˆä»Šå›ã®ä¸ä¸€è‡´åŸå› ã¸ã®å¯¾ç­–ï¼‰
# - PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆä¼šç¤¾åãƒ»è¡¨é¡Œãªã©ï¼‰ã«å«ã¾ã‚Œã‚‹æ—¥æœ¬èªã®æ¿ç‚¹ç­‰ã¯ã€
#   ç’°å¢ƒã‚„å‡¦ç†çµŒè·¯ï¼ˆHTMLâ†’æ–‡å­—åˆ—â†’ä¿å­˜ã€OSâ†’ãƒ•ã‚¡ã‚¤ãƒ«åå–å¾—ç­‰ï¼‰ã«ã‚ˆã‚Š
#   ã€Œè¦‹ãŸç›®ã¯åŒã˜ã§ã‚‚å†…éƒ¨æ–‡å­—åˆ—ãŒé•ã†ï¼ˆåˆæˆ/åˆ†é›¢ï¼‰ã€ãŒèµ·ã“ã‚Šã†ã‚‹ã€‚
# - ã“ã‚Œã‚’é˜²ããŸã‚ã€ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ã†æ–‡å­—åˆ—ã‚’ Unicode NFKC æ­£è¦åŒ–ã—ã€
#   ã•ã‚‰ã«å®‰å…¨ãªãƒ•ã‚¡ã‚¤ãƒ«åã¸æ•´å½¢ã—ã¦ä¿å­˜ã™ã‚‹ã€‚
#
# æ³¨æ„
# - PDFæœ¬æ–‡è§£æã¯è¡Œã‚ãªã„ï¼ˆâ‘¡Aã§è¡Œã†æƒ³å®šï¼‰
# - PyMuPDFã¯ä¸è¦
# - TDnetã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ—¥æ•°åˆ†å¢—ãˆã‚‹ï¼ˆãƒšãƒ¼ã‚¸æ•°Ã—æ—¥æ•° + PDFæœ¬æ•°ï¼‰
# - å¤§é‡ã«å›ã™å ´åˆã¯ PAGE_SLEEP_SEC / PDF_SLEEP_SEC ã‚’å¢—ã‚„ã™ã®ãŒå®‰å…¨
# ============================================================

import os
import datetime
import requests
import pandas as pd
import time
import re
import unicodedata
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import argparse
from pathlib import Path


# -----------------------------
# è¨­å®š
# -----------------------------
# å¯¾è±¡æŒ‡å®šï¼ˆä»¥ä¸‹ã®ã„ãšã‚Œã‹ï¼‰
# TARGET_SPEC = "20260109"               # æ—¥åˆ¥
# TARGET_SPEC = "202601"                 # æœˆæŒ‡å®š
# TARGET_SPEC = "20260105 20260109"      # ç¯„å›²æŒ‡å®šï¼ˆfrom toï¼‰
DEFAULT_TARGET_SPEC = "20260203"


# ä¿å­˜å…ˆï¼ˆWindowsãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
# ä¾‹: G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads
DEFAULT_SAVE_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"

# TDnetè² è·è»½æ¸›
PAGE_SLEEP_SEC = 3   # ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—ã”ã¨ã«å¾…æ©Ÿ
PDF_SLEEP_SEC = 1    # PDF1æœ¬DLæˆåŠŸã”ã¨ã«å¾…æ©Ÿ

# ã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚ŒãŸã‚‰å®Œå…¨é™¤å¤–ï¼ˆãƒªã‚¹ãƒˆã«ã‚‚å…¥ã‚Œãªã„ãƒ»PDFã‚‚å–ã‚‰ãªã„ï¼‰
# ä¾‹ï¼šETF/ETNãªã©ä¸è¦ãªæ—¥æ¬¡é–‹ç¤ºã‚’æ’é™¤
EXCLUDE_KEYWORDS = ["ï¼¥ï¼´ï¼¦", "ETF", "ETN", "ï¼¥ï¼´ï¼®","_MAXIS"]

# åˆ†é¡ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
# å·¦ã»ã©å„ªå…ˆåº¦ãŒé«˜ã„ï¼ˆCSVã‚½ãƒ¼ãƒˆã«ä½¿ç”¨ï¼‰
PRIORITY_KEYWORDS = ["äº‹æ¥­è¨ˆç”»", "äºˆæƒ³ã®ä¿®æ­£", "æ±ºç®—çŸ­ä¿¡", "èª¬æ˜è³‡æ–™", "æœˆæ¬¡", "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡"]

# æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹å ´åˆã®æ‰±ã„
# True ãªã‚‰ã€åŒåPDFãŒæ—¢ã«ã‚ã‚Œã°å†DLã—ãªã„ï¼ˆåŸºæœ¬ã¯ã“ã‚Œã§å®‰å…¨ï¼‰
SKIP_IF_EXISTS = True

# 1æ—¥ãƒ•ã‚©ãƒ«ãƒ€ã‚’äº‹å‰ã«ã‚¯ãƒªãƒ¼ãƒ³ã«ã™ã‚‹ã‹ï¼ˆé€šå¸¸ã¯Falseæ¨å¥¨ï¼‰
# Trueã«ã™ã‚‹ã¨ã€ãã®æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®PDF/CSVã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å–ã‚Šç›´ã™
# ï¼ˆæ®‹éª¸æ··åœ¨ã‚’çµ¶å¯¾ã«é¿ã‘ãŸã„å ´åˆã®ã¿ä½¿ã†ï¼‰
CLEAN_DAY_FOLDER = False


# -----------------------------
# Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰
# -----------------------------
def nfkc(s: str) -> str:
    """
    Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰
    - å…¨è§’/åŠè§’ã®æºã‚Œ
    - æ¿ç‚¹ã®åˆæˆ/åˆ†é›¢
    - ä¸€éƒ¨äº’æ›æ–‡å­—
    ãªã©ã‚’æƒãˆã‚‹ç›®çš„ã€‚
    """
    return unicodedata.normalize("NFKC", str(s))

# -----------------------------
# æ—¥ä»˜æŒ‡å®šã®ãƒ‘ãƒ¼ã‚¹
# -----------------------------
def parse_target_spec(spec: str):
    """
    å…¥åŠ›:
      - "YYYYMMDD"
      - "YYYYMM"
      - "YYYYMMDD YYYYMMDD" ï¼ˆfrom toï¼‰
    å‡ºåŠ›:
      (from_yyyymmdd, to_yyyymmdd, label, mode)
    """
    spec = spec.strip()
    parts = spec.split()

    if len(parts) == 1:
        s = parts[0]
        if re.fullmatch(r"\d{8}", s):
            return s, s, s, "day"
        if re.fullmatch(r"\d{6}", s):
            y = int(s[:4]); m = int(s[4:6])
            start = datetime.date(y, m, 1)
            if m == 12:
                end = datetime.date(y + 1, 1, 1) - datetime.timedelta(days=1)
            else:
                end = datetime.date(y, m + 1, 1) - datetime.timedelta(days=1)
            return start.strftime("%Y%m%d"), end.strftime("%Y%m%d"), s, "month"
        raise ValueError("TARGET_SPEC ã¯ 'YYYYMMDD' / 'YYYYMM' / 'YYYYMMDD YYYYMMDD' ã®ã„ãšã‚Œã‹ã§ã™ã€‚")

    if len(parts) == 2:
        d1, d2 = parts[0], parts[1]
        if not (re.fullmatch(r"\d{8}", d1) and re.fullmatch(r"\d{8}", d2)):
            raise ValueError("ç¯„å›²æŒ‡å®šã¯ 'YYYYMMDD YYYYMMDD' å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        if d1 > d2:
            d1, d2 = d2, d1
        return d1, d2, f"{d1}_{d2}", "range"

    raise ValueError("TARGET_SPEC ã®æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚")


def iter_dates_yyyymmdd(d_from: str, d_to: str):
    """YYYYMMDDã®ç¯„å›²ã§æ—¥ä»˜ã‚’åˆ—æŒ™ï¼ˆä¸¡ç«¯å«ã‚€ï¼‰"""
    start = datetime.datetime.strptime(d_from, "%Y%m%d").date()
    end = datetime.datetime.strptime(d_to, "%Y%m%d").date()
    cur = start
    while cur <= end:
        yield cur.strftime("%Y%m%d")
        cur += datetime.timedelta(days=1)


# -----------------------------
# åˆ†é¡ãƒ»é™¤å¤–ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«åæ•´å½¢
# -----------------------------
def get_category_score(title: str):
    """
    PRIORITY_KEYWORDS ã«å«ã¾ã‚Œã‚‹æœ€åˆã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§åˆ†é¡ã€‚
    ãƒ’ãƒƒãƒˆã—ãªã„å ´åˆã¯ã€Œãã®ä»–ã€æ‰±ã„ã€‚
    """
    for i, kw in enumerate(PRIORITY_KEYWORDS):
        if kw in title:
            return i, kw
    return 999, "ãã®ä»–"


def is_excluded(title: str) -> bool:
    """
    EXCLUDE_KEYWORDS ãŒã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚Œã‚‹å ´åˆã¯å®Œå…¨é™¤å¤–ã€‚
    æ³¨æ„:
    - ã“ã“ã¯ã‚¿ã‚¤ãƒˆãƒ«æ–‡å­—åˆ—å´ã®æ­£è¦åŒ–ã‚‚è¡Œã†ï¼ˆå…¨è§’/åŠè§’æºã‚Œå¯¾ç­–ï¼‰
    """
    if not EXCLUDE_KEYWORDS:
        return False
    t = nfkc(title)
    return any(nfkc(k) in t for k in EXCLUDE_KEYWORDS)


def safe_filename(s: str, max_len: int = 120) -> str:
    """
    Drive/Windows/ä¸€èˆ¬ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§å®‰å…¨ã«æ‰±ãˆã‚‹ã‚ˆã†ã«ãƒ•ã‚¡ã‚¤ãƒ«åã‚’æ•´å½¢ã™ã‚‹ã€‚

    è¡Œã†ã“ã¨
    - Unicode NFKC æ­£è¦åŒ–ï¼ˆæ¿ç‚¹åˆæˆ/åˆ†é›¢ãªã©ã‚’çµ±ä¸€ï¼‰
    - ç¦å‰‡æ–‡å­—ã‚’ "_" ã«ç½®æ›:  \\ / : * ? " < > |
    - é€£ç¶šç©ºç™½ã‚’æ•´ç†ï¼ˆã‚¹ãƒšãƒ¼ã‚¹1å€‹ã«ï¼‰
    - å‰å¾Œç©ºç™½ã‚’å‰Šé™¤
    - é•·ã™ãã‚‹å ´åˆã¯åˆ‡ã‚Šè©°ã‚
    """
    s = nfkc(s)
    s = re.sub(r'[\\/:*?"<>|]', "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    if len(s) > max_len:
        s = s[:max_len].rstrip()
    return s


# -----------------------------
# PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# -----------------------------
def download_pdf(session: requests.Session, url: str, save_path: str, headers: dict, cookies: dict) -> bool:
    """
    PDFã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ä¿å­˜ã€‚
    å¤±æ•—ã—ãŸå ´åˆã¯Falseã‚’è¿”ã™ï¼ˆä¾‹å¤–ã¯æ¡ã‚Šã¤ã¶ã•ãšãƒ­ã‚°è¡¨ç¤ºï¼‰ã€‚
    """
    try:
        r = session.get(url, headers=headers, cookies=cookies, stream=True, timeout=60)
        r.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024 * 256):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        print(f"   âŒ PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return False


# -----------------------------
# ï¼ˆä»»æ„ï¼‰æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
# -----------------------------
def cleanup_day_folder(day_dir: str):
    """
    ãã®æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€é…ä¸‹ã®PDF/CSVã‚’å‰Šé™¤ã™ã‚‹ã€‚
    - æ®‹éª¸æ··åœ¨ã‚’çµ¶å¯¾ã«é¿ã‘ãŸã„å ´åˆã®ã¿åˆ©ç”¨
    """
    if not os.path.isdir(day_dir):
        return
    for fn in os.listdir(day_dir):
        p = os.path.join(day_dir, fn)
        if os.path.isfile(p) and (fn.lower().endswith(".pdf") or fn.lower().endswith(".csv")):
            try:
                os.remove(p)
            except Exception:
                pass


# -----------------------------
# å¼•æ•°
# -----------------------------
def parse_args():
    p = argparse.ArgumentParser(description="TDnetä¸€è¦§å–å¾—ï¼†PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆæ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ä¿å­˜ï¼‰")
    p.add_argument("--target", default=DEFAULT_TARGET_SPEC, help="YYYYMMDD / YYYYMM / 'YYYYMMDD YYYYMMDD'")
    p.add_argument("--save-root", default=DEFAULT_SAVE_ROOT, help="ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ï¼ˆä¾‹: G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\TDnet_Downloadsï¼‰")
    p.add_argument("--page-sleep", type=float, default=PAGE_SLEEP_SEC, help="ä¸€è¦§ãƒšãƒ¼ã‚¸å–å¾—ã”ã¨ã®å¾…æ©Ÿç§’")
    p.add_argument("--pdf-sleep", type=float, default=PDF_SLEEP_SEC, help="PDF1æœ¬ä¿å­˜ã”ã¨ã®å¾…æ©Ÿç§’")
    p.add_argument("--skip-if-exists", action="store_true", default=SKIP_IF_EXISTS, help="åŒåPDFãŒæ—¢ã«ã‚ã‚Œã°å†DLã—ãªã„")
    p.add_argument("--no-skip-if-exists", dest="skip_if_exists", action="store_false", help="åŒåPDFãŒã‚ã£ã¦ã‚‚å†DLã™ã‚‹")
    p.add_argument("--clean-day-folder", action="store_true", default=CLEAN_DAY_FOLDER, help="æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã®PDF/CSVã‚’å‰Šé™¤ã—ã¦ã‹ã‚‰å–å¾—")
    return p.parse_args()


# -----------------------------
# ãƒ¡ã‚¤ãƒ³ï¼šæŒ‡å®šç¯„å›²ã‚’æ—¥ã”ã¨ã«å‡¦ç†
# -----------------------------
def main():
    args = parse_args()
    target_spec = args.target
    save_root = Path(args.save_root)
    save_root.mkdir(parents=True, exist_ok=True)

    d_from, d_to, label, mode = parse_target_spec(target_spec)

    print(f"ğŸ¯ å¯¾è±¡æŒ‡å®š: {target_spec}ï¼ˆmode={mode}, from={d_from}, to={d_to}ï¼‰")
    print(f"ğŸ“ ä¿å­˜ãƒ«ãƒ¼ãƒˆ: {save_root}")

    base_url_template = "https://www.release.tdnet.info/inbs/I_list_{}_{}.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Referer": "https://www.release.tdnet.info/index.html",
    }
    cookies = {"cb_agree": "0"}
    session = requests.Session()

    # å…¨æœŸé–“ã®çµ±è¨ˆ
    total_page_access = 0
    total_pdf_success = 0
    total_excluded = 0
    days_with_no_data = []

    for target_date_str in iter_dates_yyyymmdd(d_from, d_to):
        print("\n" + "=" * 60)
        print(f"ğŸ“… æ—¥ä»˜: {target_date_str} ã‚’å‡¦ç†ã—ã¾ã™")

        # æ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€
        day_dir = save_root / target_date_str
        day_dir.mkdir(parents=True, exist_ok=True)

        # å¿…è¦ãªã‚‰ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆé€šå¸¸ã¯Falseï¼‰
        if args.clean_day_folder:
            print("   ğŸ§¹ æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã™ï¼ˆPDF/CSVå‰Šé™¤ï¼‰")
            cleanup_day_folder(str(day_dir))

        data_list = []
        page_num = 1

        day_page_access = 0
        day_pdf_success = 0
        day_excluded = 0

        while True:
            page_str = f"{page_num:03}"
            target_url = base_url_template.format(page_str, target_date_str)

            print(f"   ...Page {page_str} ã‚’ç¢ºèªä¸­")
            res = session.get(target_url, headers=headers, cookies=cookies, timeout=60)
            day_page_access += 1
            res.encoding = "utf-8"

            # ãƒ‡ãƒ¼ã‚¿ç„¡ã—åˆ¤å®š
            if res.status_code == 404 or "è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“" in res.text:
                if page_num == 1:
                    print("   âš ï¸ è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆä¼‘æ—¥ç­‰ã®å¯èƒ½æ€§ï¼‰")
                    days_with_no_data.append(target_date_str)
                break

            soup = BeautifulSoup(res.text, "html.parser")
            rows = soup.find_all("tr")

            # TDnetå´ã®HTMLãŒæƒ³å®šã‚ˆã‚Šå°‘ãªã„å ´åˆã¯çµ‚äº†
            if len(rows) < 5:
                break

            for row in rows:
                cols = row.find_all("td")
                if len(cols) < 5:
                    continue

                # å–å¾—æ–‡å­—åˆ—ã¯ã€å¾Œæ®µã§NFKCæ­£è¦åŒ–ã—ã¦æºã‚Œã‚’å¸å
                r_time = nfkc(cols[0].get_text(strip=True))
                r_code = nfkc(cols[1].get_text(strip=True))  # 4æ¡æ•°å­—ã¨ã¯é™ã‚‰ãªã„ï¼ˆä¾‹: 137Aï¼‰
                r_name = nfkc(cols[2].get_text(strip=True))
                r_title = nfkc(cols[3].get_text(strip=True))

                # é™¤å¤–ï¼ˆå®Œå…¨ã‚¹ã‚­ãƒƒãƒ—ï¼šCSVã«ã‚‚å…¥ã‚Œãªã„ã—PDFã‚‚å–ã‚‰ãªã„ï¼‰
                if is_excluded(r_title):
                    day_excluded += 1
                    continue

                # PDFãƒªãƒ³ã‚¯å–å¾—ï¼ˆãƒªãƒ³ã‚¯ãŒå–ã‚Œãªã„è¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                link_tag = cols[3].find("a")
                if not link_tag:
                    link_tag = cols[4].find("a")
                if not link_tag:
                    continue

                pdf_link = urljoin(target_url, link_tag.get("href"))

                # åˆ†é¡
                score, category_name = get_category_score(r_title)

                # PDFãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
                t = r_time.replace(":", "")
                code4 = (r_code[:4] or "").strip()

                fn = (
                    f"{safe_filename(code4, max_len=4)}_"
                    f"{safe_filename(t, max_len=10)}_"
                    f"{safe_filename(r_name)}_"
                    f"{safe_filename(r_title)}.pdf"
                )
                pdf_path = day_dir / fn

                # PDFä¿å­˜ï¼ˆæ—¢å­˜ãŒã‚ã‚Œã°ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                need_download = True
                if args.skip_if_exists and pdf_path.exists():
                    need_download = False

                if need_download:
                    ok = download_pdf(session, pdf_link, str(pdf_path), headers, cookies)
                    if ok:
                        day_pdf_success += 1
                        print(f"   âœ… ä¿å­˜: {fn}")
                        if args.pdf_sleep > 0:
                            time.sleep(args.pdf_sleep)

                # ä¸€è¦§CSVç”¨ï¼ˆé™¤å¤–ä»¥å¤–ã¯å…¨ä»¶å…¥ã‚Œã‚‹ï¼‰
                sheet_link = f'=HYPERLINK("{pdf_link}", "{r_title}")'
                data_list.append(
                    {
                        "å„ªå…ˆåº¦": score,
                        "åˆ†é¡": category_name,
                        "æ™‚åˆ»": r_time,
                        "ã‚³ãƒ¼ãƒ‰": code4,
                        "ä¼šç¤¾å": r_name,
                        "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰": sheet_link,
                        "URLï¼ˆç”Ÿï¼‰": pdf_link,
                        "PDFãƒ•ã‚¡ã‚¤ãƒ«å": fn,
                    }
                )

            page_num += 1
            if args.page_sleep > 0:
                time.sleep(args.page_sleep)

        # æ—¥åˆ¥CSVä¿å­˜
        if data_list:
            df = pd.DataFrame(data_list)

            # å„ªå…ˆåº¦ï¼ˆå°ã•ã„ã»ã©å„ªå…ˆï¼‰â†’ æ™‚åˆ»ï¼ˆæ–°ã—ã„é †ï¼‰ã§ä¸¦ã¹ã‚‹
            df_sorted = df.sort_values(by=["å„ªå…ˆåº¦", "æ™‚åˆ»"], ascending=[True, False])
            df_final = df_sorted[["åˆ†é¡", "æ™‚åˆ»", "ã‚³ãƒ¼ãƒ‰", "ä¼šç¤¾å", "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "URLï¼ˆç”Ÿï¼‰", "PDFãƒ•ã‚¡ã‚¤ãƒ«å"]]

            out_csv = f"TDnet_Sorted_{target_date_str}.csv"

            # â‘  æ—¥ä»˜åˆ¥ãƒ•ã‚©ãƒ«ãƒ€ã«ä¿å­˜
            out_path = day_dir / out_csv
            df_final.to_csv(out_path, index=False, encoding="utf-8-sig")
            print(f"   ğŸ“ ä¸€è¦§CSVä¿å­˜: {out_csv}")

            # â‘¡ ãƒ«ãƒ¼ãƒˆãƒ•ã‚©ãƒ«ãƒ€ã«ã‚‚ã‚³ãƒ”ãƒ¼ä¿å­˜ï¼ˆâ‘¡Bã§å‚ç…§ã§ãã‚‹ã‚ˆã†ã«ï¼‰
            out_path_root = save_root / out_csv
            df_final.to_csv(out_path_root, index=False, encoding="utf-8-sig")
            print(f"   ğŸ“ ä¸€è¦§CSVä¿å­˜ï¼ˆãƒ«ãƒ¼ãƒˆï¼‰: {out_csv}")
        else:
            print("   ğŸ“ ä¸€è¦§CSV: ï¼ˆä½œæˆãªã—ï¼‰")

        # æ—¥åˆ¥çµ±è¨ˆ
        print(f"   ğŸ“Š æ—¥åˆ¥çµ±è¨ˆ: page_access={day_page_access}, pdf_success={day_pdf_success}, excluded={day_excluded}")

        # æœŸé–“åˆç®—
        total_page_access += day_page_access
        total_pdf_success += day_pdf_success
        total_excluded += day_excluded

    print("\n" + "=" * 60)
    print("âœ… â‘ å®Œäº†ï¼ˆç¯„å›²å–å¾—ï¼‰")
    print(f"   æœŸé–“: {d_from} ï½ {d_to} ï¼ˆmode={mode}ï¼‰")
    print(f"   ä¸€è¦§ãƒšãƒ¼ã‚¸ã‚¢ã‚¯ã‚»ã‚¹åˆè¨ˆ: {total_page_access}")
    print(f"   PDFãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æˆåŠŸåˆè¨ˆ: {total_pdf_success}")
    print(f"   é™¤å¤–ä»¶æ•°åˆè¨ˆ: {total_excluded}")
    if days_with_no_data:
        print(f"   ãƒ‡ãƒ¼ã‚¿ãªã—æ—¥: {', '.join(days_with_no_data)}")
    print(f"   ä¿å­˜ãƒ«ãƒ¼ãƒˆ: {save_root}")


if __name__ == "__main__":
    main()
