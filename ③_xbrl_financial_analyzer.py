# -*- coding: utf-8 -*-
"""
â‘¢ XBRL Financial Analyzer â€” TDnet XBRLå–å¾— & è²¡å‹™åˆ†æãƒ„ãƒ¼ãƒ«

ã€æ©Ÿèƒ½æ¦‚è¦ã€‘
1. TDnetã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰XBRLãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆZIPï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
2. XBRLã‚’è§£æã—ã€è²¡å‹™è«¸è¡¨ãƒ‡ãƒ¼ã‚¿ï¼ˆP/L, B/S, CFï¼‰ã‚’æŠ½å‡º
3. Excelãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ï¼ˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã§æ“ä½œå¯èƒ½ãªå½¢å¼ï¼‰
4. è²¡å‹™åˆ†æã‚’å®Ÿè¡Œ:
   - å‰æœŸæ¯”ã®å¢—æ¸›ç‡è¨ˆç®—
   - å¤§ããå¤‰åŒ–ã—ãŸå‹˜å®šç§‘ç›®ã®æ¤œå‡º
   - å£²ä¸Šé«˜åˆ©ç›Šç‡ï¼ˆå–¶æ¥­åˆ©ç›Šç‡ã€çµŒå¸¸åˆ©ç›Šç‡ã€ç´”åˆ©ç›Šç‡ï¼‰ã®è¨ˆç®—

ã€å®Ÿè¡Œä¾‹ã€‘
  python "â‘¢_xbrl_financial_analyzer.py" --target "20260202"
  python "â‘¢_xbrl_financial_analyzer.py" --target "20260202" --code 7203
  python "â‘¢_xbrl_financial_analyzer.py" --target "202602" --threshold 0.15

ã€å‰æã€‘
  pip install requests beautifulsoup4 lxml pandas openpyxl
"""

import os
import sys
import datetime
import requests
import pandas as pd
import time
import re
import unicodedata
import argparse
import zipfile
import io
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
from lxml import etree
from openpyxl import Workbook
from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
from openpyxl.utils import get_column_letter

# Windows cp932 ã§çµµæ–‡å­—ãŒå‡ºåŠ›ã§ããªã„å•é¡Œã®å›é¿
try:
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
except Exception:
    pass


# ============================================================
# å®šæ•°ãƒ»è¨­å®š
# ============================================================

DEFAULT_TARGET_SPEC = "20260203"

# ä¿å­˜å…ˆï¼ˆWindowsãƒ­ãƒ¼ã‚«ãƒ«ï¼‰
DEFAULT_SAVE_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_XBRL"

# TDnetè² è·è»½æ¸›
PAGE_SLEEP_SEC = 3
XBRL_SLEEP_SEC = 1

# å¤‰åŒ–ç‡ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ20%ä»¥ä¸Šã®å¤‰åŒ–ã‚’ã€Œå¤§ããªå¤‰å‹•ã€ã¨ã™ã‚‹ï¼‰
DEFAULT_CHANGE_THRESHOLD = 0.20

# é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«å«ã¾ã‚ŒãŸã‚‰å®Œå…¨é™¤å¤–ï¼‰
EXCLUDE_KEYWORDS = ["ï¼¥ï¼´ï¼¦", "ETF", "ETN", "ï¼¥ï¼´ï¼®", "_MAXIS"]

# TDnet URL template
BASE_URL_TEMPLATE = "https://www.release.tdnet.info/inbs/I_list_{}_{}.html"

# HTTPè¨­å®š
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
    "Referer": "https://www.release.tdnet.info/index.html",
}
COOKIES = {"cb_agree": "0"}


# ============================================================
# XBRLãƒ©ãƒ™ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆè¦ç´ å â†’ æ—¥æœ¬èªåç§°ï¼‰
# ============================================================

XBRL_LABEL_MAP = {
    # --- æç›Šè¨ˆç®—æ›¸ (P/L) ---
    "NetSales": "å£²ä¸Šé«˜",
    "Revenue": "å£²ä¸Šåç›Šï¼ˆIFRSï¼‰",
    "OperatingRevenue1": "å–¶æ¥­åç›Š",
    "CostOfSales": "å£²ä¸ŠåŸä¾¡",
    "GrossProfit": "å£²ä¸Šç·åˆ©ç›Š",
    "SellingGeneralAndAdministrativeExpenses": "è²©å£²è²»åŠã³ä¸€èˆ¬ç®¡ç†è²»",
    "OperatingIncome": "å–¶æ¥­åˆ©ç›Š",
    "NonOperatingIncome": "å–¶æ¥­å¤–åç›Š",
    "NonOperatingExpenses": "å–¶æ¥­å¤–è²»ç”¨",
    "OrdinaryIncome": "çµŒå¸¸åˆ©ç›Š",
    "ExtraordinaryIncome": "ç‰¹åˆ¥åˆ©ç›Š",
    "ExtraordinaryLoss": "ç‰¹åˆ¥æå¤±",
    "IncomeBeforeIncomeTaxes": "ç¨å¼•å‰å½“æœŸç´”åˆ©ç›Š",
    "IncomeTaxes": "æ³•äººç¨ç­‰åˆè¨ˆ",
    "ProfitLoss": "å½“æœŸç´”åˆ©ç›Š",
    "ProfitLossAttributableToOwnersOfParent": "è¦ªä¼šç¤¾æ ªä¸»ã«å¸°å±ã™ã‚‹å½“æœŸç´”åˆ©ç›Š",
    "ComprehensiveIncome": "åŒ…æ‹¬åˆ©ç›Š",

    # --- è²¸å€Ÿå¯¾ç…§è¡¨ (B/S) ---
    "CurrentAssets": "æµå‹•è³‡ç”£åˆè¨ˆ",
    "NoncurrentAssets": "å›ºå®šè³‡ç”£åˆè¨ˆ",
    "DeferredAssets": "ç¹°å»¶è³‡ç”£",
    "TotalAssets": "ç·è³‡ç”£",
    "CurrentLiabilities": "æµå‹•è² å‚µåˆè¨ˆ",
    "NoncurrentLiabilities": "å›ºå®šè² å‚µåˆè¨ˆ",
    "TotalLiabilities": "è² å‚µåˆè¨ˆ",
    "NetAssets": "ç´”è³‡ç”£åˆè¨ˆ",
    "ShareholdersEquity": "æ ªä¸»è³‡æœ¬åˆè¨ˆ",
    "CapitalStock": "è³‡æœ¬é‡‘",
    "CapitalSurplus": "è³‡æœ¬å‰°ä½™é‡‘",
    "RetainedEarnings": "åˆ©ç›Šå‰°ä½™é‡‘",
    "TreasuryStock": "è‡ªå·±æ ªå¼",

    # --- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼è¨ˆç®—æ›¸ (CF) ---
    "NetCashProvidedByUsedInOperatingActivities": "å–¶æ¥­æ´»å‹•ã«ã‚ˆã‚‹CF",
    "NetCashProvidedByUsedInInvestingActivities": "æŠ•è³‡æ´»å‹•ã«ã‚ˆã‚‹CF",
    "NetCashProvidedByUsedInFinancingActivities": "è²¡å‹™æ´»å‹•ã«ã‚ˆã‚‹CF",
    "CashAndCashEquivalents": "ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©æœŸæœ«æ®‹é«˜",
    "IncreaseDecreaseInCashAndCashEquivalents": "ç¾é‡‘åŠã³ç¾é‡‘åŒç­‰ç‰©ã®å¢—æ¸›é¡",

    # --- 1æ ªå½“ãŸã‚Šæƒ…å ± ---
    "EarningsPerShare": "1æ ªå½“ãŸã‚Šå½“æœŸç´”åˆ©ç›Š",
    "DilutedEarningsPerShare": "æ½œåœ¨æ ªå¼èª¿æ•´å¾ŒEPS",
    "DividendPerShare": "1æ ªå½“ãŸã‚Šé…å½“é¡",
    "NetAssetsPerShare": "1æ ªå½“ãŸã‚Šç´”è³‡ç”£",

    # --- çµŒå–¶æŒ‡æ¨™ ---
    "EquityToAssetRatio": "è‡ªå·±è³‡æœ¬æ¯”ç‡ï¼ˆ%ï¼‰",
    "RateOfReturnOnEquity": "è‡ªå·±è³‡æœ¬åˆ©ç›Šç‡ROEï¼ˆ%ï¼‰",
    "PriceEarningsRatio": "æ ªä¾¡åç›Šç‡PERï¼ˆå€ï¼‰",

    # --- ä¼šç¤¾æƒ…å ± (DEI) ---
    "FilerNameInJapaneseDEI": "æå‡ºè€…å",
    "SecurityCodeDEI": "è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰",
    "AccountingStandardsDEI": "ä¼šè¨ˆåŸºæº–",
    "CurrentFiscalYearStartDateDEI": "å½“æœŸé–‹å§‹æ—¥",
    "CurrentFiscalYearEndDateDEI": "å½“æœŸçµ‚äº†æ—¥",
    "CurrentPeriodEndDateDEI": "å½“å››åŠæœŸæœ«æ—¥",
    "TypeOfCurrentPeriodDEI": "å½“å››åŠæœŸä¼šè¨ˆæœŸé–“ã®ç¨®é¡",
}


# ============================================================
# TDnetã‚µãƒãƒªãƒ¼è¦ç´ å â†’ æ¨™æº–è¦ç´ åãƒãƒƒãƒ”ãƒ³ã‚°
# TDnetã®æ±ºç®—çŸ­ä¿¡ã‚µãƒãƒªãƒ¼ã¯ tse-ed-t åå‰ç©ºé–“ç‹¬è‡ªã®è¦ç´ åã‚’ä½¿ã†
# ============================================================

TSE_ELEMENT_MAP = {
    # --- P/L æ—¥æœ¬åŸºæº– ---
    "NetSales": "NetSales",
    "OperatingIncome": "OperatingIncome",
    "OrdinaryIncome": "OrdinaryIncome",
    "ProfitLoss": "ProfitLoss",
    "ProfitLossAttributableToOwnersOfParent": "ProfitLossAttributableToOwnersOfParent",
    "ComprehensiveIncome": "ComprehensiveIncome",
    # --- P/L IFRS ---
    "SalesIFRS": "NetSales",
    "RevenueIFRS": "NetSales",
    "OperatingIncomeIFRS": "OperatingIncome",
    "ProfitBeforeTaxIFRS": "IncomeBeforeIncomeTaxes",
    "ProfitLossIFRS": "ProfitLoss",
    "ProfitLossAttributableToOwnersOfParentIFRS": "ProfitLossAttributableToOwnersOfParent",
    "ComprehensiveIncomeIFRS": "ComprehensiveIncome",
    # --- å¤‰å‹•ç‡ ---
    "ChangeInNetSales": "ChangeInNetSales",
    "ChangeInOperatingIncome": "ChangeInOperatingIncome",
    "ChangeInOrdinaryIncome": "ChangeInOrdinaryIncome",
    "ChangeInProfitLoss": "ChangeInProfitLoss",
    "ChangeInSalesIFRS": "ChangeInNetSales",
    "ChangeInOperatingIncomeIFRS": "ChangeInOperatingIncome",
    "ChangeInProfitBeforeTaxIFRS": "ChangeInIncomeBeforeTaxes",
    "ChangeInProfitLossIFRS": "ChangeInProfitLoss",
    # --- EPS ---
    "EarningsPerShare": "EarningsPerShare",
    "DilutedEarningsPerShare": "DilutedEarningsPerShare",
    "EarningsPerShareIFRS": "EarningsPerShare",
    "DilutedEarningsPerShareIFRS": "DilutedEarningsPerShare",
    # --- B/S ---
    "TotalAssets": "TotalAssets",
    "NetAssets": "NetAssets",
    "Equity": "ShareholdersEquity",
    "TotalAssetsIFRS": "TotalAssets",
    "NetAssetsIFRS": "NetAssets",
    "EquityIFRS": "ShareholdersEquity",
    "EquityToAssetRatio": "EquityToAssetRatio",
    "EquityToAssetRatioIFRS": "EquityToAssetRatio",
    "BookValuePerShare": "NetAssetsPerShare",
    "BookValuePerShareIFRS": "NetAssetsPerShare",
    # --- é…å½“ ---
    "DividendPerShare": "DividendPerShare",
    "AnnualDividendPerShare": "DividendPerShare",
    "DividendPerShareIFRS": "DividendPerShare",
    # --- CF ---
    "CashFlowsFromOperatingActivities": "NetCashProvidedByUsedInOperatingActivities",
    "CashFlowsFromInvestingActivities": "NetCashProvidedByUsedInInvestingActivities",
    "CashFlowsFromFinancingActivities": "NetCashProvidedByUsedInFinancingActivities",
    "CashAndEquivalents": "CashAndCashEquivalents",
    "CashFlowsFromOperatingActivitiesIFRS": "NetCashProvidedByUsedInOperatingActivities",
    "CashFlowsFromInvestingActivitiesIFRS": "NetCashProvidedByUsedInInvestingActivities",
    "CashFlowsFromFinancingActivitiesIFRS": "NetCashProvidedByUsedInFinancingActivities",
    "CashAndEquivalentsIFRS": "CashAndCashEquivalents",
    # --- IFRSè¿½åŠ è¦ç´  ---
    "ProfitIFRS": "ProfitLoss",
    "ProfitAttributableToOwnersOfParentIFRS": "ProfitLossAttributableToOwnersOfParent",
    "TotalComprehensiveIncomeIFRS": "ComprehensiveIncome",
    "BasicEarningsPerShareIFRS": "EarningsPerShare",
    "TotalEquityIFRS": "NetAssets",
    "EquityAttributableToOwnersOfParentIFRS": "ShareholdersEquity",
    "EquityAttributableToOwnersOfParentToTotalAssetsRatioIFRS": "EquityToAssetRatio",
}


# ============================================================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆâ‘ ã¨å…±é€šï¼‰
# ============================================================

def nfkc(s: str) -> str:
    """Unicodeæ­£è¦åŒ–ï¼ˆNFKCï¼‰"""
    return unicodedata.normalize("NFKC", str(s))


def safe_filename(s: str, max_len: int = 120) -> str:
    """ãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ãˆã‚‹å®‰å…¨ãªæ–‡å­—åˆ—ã«å¤‰æ›"""
    s = nfkc(s)
    s = re.sub(r'[\\/:*?"<>|]', "_", s)
    s = re.sub(r"\s+", " ", s).strip()
    if len(s) > max_len:
        s = s[:max_len].rstrip()
    return s


def is_excluded(title: str) -> bool:
    """é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«è©²å½“ã™ã‚‹ã‹åˆ¤å®š"""
    if not EXCLUDE_KEYWORDS:
        return False
    t = nfkc(title)
    return any(nfkc(k) in t for k in EXCLUDE_KEYWORDS)


def parse_target_spec(spec: str):
    """æ—¥ä»˜æŒ‡å®šã®ãƒ‘ãƒ¼ã‚¹ï¼ˆâ‘ ã¨åŒä¸€ä»•æ§˜ï¼‰"""
    spec = spec.strip()
    parts = spec.split()

    if len(parts) == 1:
        s = parts[0]
        if re.fullmatch(r"\d{8}", s):
            return s, s, s, "day"
        if re.fullmatch(r"\d{6}", s):
            y = int(s[:4]); m = int(s[4:6])
            start = datetime.date(y, m, 1)
            if m == 12:
                end = datetime.date(y + 1, 1, 1) - datetime.timedelta(days=1)
            else:
                end = datetime.date(y, m + 1, 1) - datetime.timedelta(days=1)
            return start.strftime("%Y%m%d"), end.strftime("%Y%m%d"), s, "month"
        raise ValueError("TARGET_SPEC ã¯ 'YYYYMMDD' / 'YYYYMM' / 'YYYYMMDD YYYYMMDD' ã®ã„ãšã‚Œã‹ã§ã™ã€‚")

    if len(parts) == 2:
        d1, d2 = parts
        if not (re.fullmatch(r"\d{8}", d1) and re.fullmatch(r"\d{8}", d2)):
            raise ValueError("ç¯„å›²æŒ‡å®šã¯ 'YYYYMMDD YYYYMMDD' å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        if d1 > d2:
            d1, d2 = d2, d1
        return d1, d2, f"{d1}_{d2}", "range"

    raise ValueError("TARGET_SPEC ã®æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚")


def iter_dates_yyyymmdd(d_from: str, d_to: str):
    """YYYYMMDDã®ç¯„å›²ã§æ—¥ä»˜ã‚’åˆ—æŒ™ï¼ˆä¸¡ç«¯å«ã‚€ï¼‰"""
    start = datetime.datetime.strptime(d_from, "%Y%m%d").date()
    end = datetime.datetime.strptime(d_to, "%Y%m%d").date()
    cur = start
    while cur <= end:
        yield cur.strftime("%Y%m%d")
        cur += datetime.timedelta(days=1)


# ============================================================
# Section 1: TDnet XBRL ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
# ============================================================

def find_xbrl_links(session, target_date_str, code_filter=None):
    """
    TDnetã®ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰XBRLãƒªãƒ³ã‚¯ï¼ˆ.zipï¼‰ã‚’å–å¾—ã™ã‚‹ã€‚

    Returns:
        list of dict: [{time, code, name, title, xbrl_url}, ...]
    """
    results = []
    page_num = 1

    while True:
        page_str = f"{page_num:03}"
        target_url = BASE_URL_TEMPLATE.format(page_str, target_date_str)

        print(f"   ...Page {page_str} ã‚’ç¢ºèªä¸­")
        try:
            res = session.get(target_url, headers=HEADERS, cookies=COOKIES, timeout=60)
        except requests.RequestException as e:
            print(f"   âŒ ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            break

        res.encoding = "utf-8"

        # ãƒ‡ãƒ¼ã‚¿ç„¡ã—åˆ¤å®š
        if res.status_code == 404 or "è©²å½“ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã¯ã‚ã‚Šã¾ã›ã‚“" in res.text:
            if page_num == 1:
                print("   âš ï¸ è©²å½“ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆä¼‘æ—¥ç­‰ã®å¯èƒ½æ€§ï¼‰")
            break

        soup = BeautifulSoup(res.text, "html.parser")
        rows = soup.find_all("tr")

        if len(rows) < 5:
            break

        found_in_page = 0
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5:
                continue

            r_time = nfkc(cols[0].get_text(strip=True))
            r_code = nfkc(cols[1].get_text(strip=True))
            r_name = nfkc(cols[2].get_text(strip=True))
            r_title = nfkc(cols[3].get_text(strip=True))

            # é™¤å¤–
            if is_excluded(r_title):
                continue

            # ã‚³ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿
            code4 = (r_code[:4] or "").strip()
            if code_filter and code4 != str(code_filter):
                continue

            # XBRLãƒªãƒ³ã‚¯æ¢ç´¢: å…¨ã‚«ãƒ©ãƒ ã‹ã‚‰ .zip ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            xbrl_url = None
            for col in cols:
                for a_tag in col.find_all("a", href=True):
                    href = a_tag.get("href", "")
                    if href.lower().endswith(".zip"):
                        xbrl_url = urljoin(target_url, href)
                        break
                if xbrl_url:
                    break

            if xbrl_url:
                results.append({
                    "time": r_time,
                    "code": code4,
                    "name": r_name,
                    "title": r_title,
                    "xbrl_url": xbrl_url,
                })
                found_in_page += 1

        page_num += 1
        if PAGE_SLEEP_SEC > 0:
            time.sleep(PAGE_SLEEP_SEC)

    return results


def download_xbrl_zip(session, url, save_path):
    """XBRLã®ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
    try:
        r = session.get(url, headers=HEADERS, cookies=COOKIES, stream=True, timeout=60)
        r.raise_for_status()
        with open(save_path, "wb") as f:
            for chunk in r.iter_content(chunk_size=1024 * 256):
                if chunk:
                    f.write(chunk)
        return True
    except Exception as e:
        print(f"   âŒ XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}")
        return False


# ============================================================
# Section 2: XBRL è§£æ
# ============================================================

def find_xbrl_instance_in_zip(zip_path):
    """
    ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰XBRLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¢ã™ã€‚

    TDnet XBRL ã® ZIPæ§‹é€ :
      XBRLData/Summary/   *-ixbrl.htm    â† ã‚µãƒãƒªãƒ¼ï¼ˆæ±ºç®—çŸ­ä¿¡1ãƒšãƒ¼ã‚¸ç›®ï¼‰
      XBRLData/Attachment/ *-ixbrl.htm    â† è©³ç´°ï¼ˆè²¡å‹™è«¸è¡¨: B/S, P/L, CFç­‰ï¼‰
                           *-def.xml      â† å®šç¾©ï¼ˆâ†ã“ã‚Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                           *-pre.xml      â† è¡¨ç¤ºï¼ˆâ†ã“ã‚Œã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                           *-cal.xml      â† è¨ˆç®—ï¼ˆâ†ã“ã‚Œã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰
                           *-lab.xml      â† ãƒ©ãƒ™ãƒ«ï¼ˆâ†ã“ã‚Œã‚‚ã‚¹ã‚­ãƒƒãƒ—ï¼‰

    å„ªå…ˆé †ä½:
      1. Attachment é…ä¸‹ã® -ixbrl.htmï¼ˆæœ€ã‚‚ã‚µã‚¤ã‚ºãŒå¤§ãã„ã‚‚ã® = B/Så…¨ä½“ ç­‰ï¼‰
      2. Summary é…ä¸‹ã® -ixbrl.htmï¼ˆã‚µãƒãƒªãƒ¼æƒ…å ±ï¼‰
      3. .xbrl ãƒ•ã‚¡ã‚¤ãƒ«
    """
    with zipfile.ZipFile(zip_path, 'r') as zf:
        ixbrl_attachment = []
        ixbrl_summary = []
        xbrl_files = []

        for name in zf.namelist():
            lower = name.lower()
            if lower.endswith('/'):
                continue

            info = zf.getinfo(name)

            # Inline XBRLï¼ˆãƒ¡ã‚¤ãƒ³ãƒ‡ãƒ¼ã‚¿ï¼‰
            if lower.endswith('-ixbrl.htm') or lower.endswith('-ixbrl.html'):
                if 'attachment' in lower:
                    ixbrl_attachment.append((name, info.file_size))
                elif 'summary' in lower:
                    ixbrl_summary.append((name, info.file_size))
                else:
                    ixbrl_attachment.append((name, info.file_size))

            # é€šå¸¸ã® XBRL ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
            elif lower.endswith('.xbrl'):
                xbrl_files.append((name, info.file_size))

        # ã‚µãƒãƒªãƒ¼ã® iXBRL ã‚’è¿”ã™ï¼ˆæ±ºç®—æ¦‚è¦ãƒ‡ãƒ¼ã‚¿ï¼‰
        # â†’ æœ€ã‚‚ã‚µã‚¤ã‚ºãŒå¤§ãã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ
        if ixbrl_summary:
            ixbrl_summary.sort(key=lambda x: -x[1])
            best = ixbrl_summary[0][0]
            content = zf.read(best)
            return best, content

        # Attachment ã® iXBRLï¼ˆè©³ç´°è²¡å‹™è«¸è¡¨ï¼‰
        if ixbrl_attachment:
            ixbrl_attachment.sort(key=lambda x: -x[1])
            best = ixbrl_attachment[0][0]
            content = zf.read(best)
            return best, content

        # é€šå¸¸ã® XBRL
        if xbrl_files:
            xbrl_files.sort(key=lambda x: -x[1])
            best = xbrl_files[0][0]
            content = zf.read(best)
            return best, content

        print("   âš ï¸ XBRLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return None, None


def parse_contexts(tree):
    """
    XBRLã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¦ç´ ã‚’è§£æã—ã¦è¾æ›¸ã§è¿”ã™ã€‚
    HTMLãƒ‘ãƒ¼ã‚µãƒ¼ä½¿ç”¨æ™‚ã¯ã‚¿ã‚°åãŒå°æ–‡å­—åŒ–ã•ã‚Œã‚‹ï¼ˆxbrli:context ç­‰ï¼‰ã€‚
    """
    contexts = {}

    for elem in tree.iter():
        tag_str = str(elem.tag)
        # åå‰ç©ºé–“ä»˜ã or HTMLãƒ‘ãƒ¼ã‚µãƒ¼ã§å°æ–‡å­—åŒ–ã•ã‚ŒãŸå½¢å¼
        if '}' in tag_str:
            local_name = etree.QName(tag_str).localname
        else:
            local_name = tag_str
            # HTMLãƒ‘ãƒ¼ã‚µãƒ¼ã®å ´åˆ: "xbrli:context" â†’ "context" éƒ¨åˆ†ã‚’å–å¾—
            if ':' in local_name:
                local_name = local_name.split(':', 1)[1]

        if local_name.lower() == "context":
            ctx_id = elem.get("id", "")
            if ctx_id:
                period_info = {}
                for child in elem.iter():
                    child_tag = str(child.tag)
                    if '}' in child_tag:
                        child_name = etree.QName(child_tag).localname
                    else:
                        child_name = child_tag
                        if ':' in child_name:
                            child_name = child_name.split(':', 1)[1]

                    child_lower = child_name.lower()
                    if child_lower in ("startdate", "enddate", "instant"):
                        if child.text:
                            period_info[child_lower] = child.text
                contexts[ctx_id] = period_info

    return contexts


def classify_period(context_ref: str, contexts: dict) -> str:
    """
    ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆIDã‹ã‚‰æœŸé–“ã‚¿ã‚¤ãƒ—ã‚’åˆ†é¡ã™ã‚‹ã€‚

    TDnet XBRL ã®å…¸å‹çš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆIDä¾‹:
      é€šå¸¸:
        CurrentYearDuration        â†’ å½“æœŸ
        PriorYearDuration          â†’ å‰æœŸ
        CurrentYearInstant         â†’ å½“æœŸæœ«
        PriorYearInstant           â†’ å‰æœŸæœ«
      TDnetã‚µãƒãƒªãƒ¼:
        CurrentAccumulatedQ3Duration_ConsolidatedMember_ResultMember â†’ å½“æœŸ
        PriorAccumulatedQ3Duration_ConsolidatedMember_ResultMember   â†’ å‰æœŸ
        CurrentAccumulatedQ3Instant                                  â†’ å½“æœŸæœ«
        PriorAccumulatedQ3Instant                                    â†’ å‰æœŸæœ«
        NextAccumulatedFYDuration_ConsolidatedMember_ForecastMember  â†’ äºˆæƒ³
    """
    cr = context_ref.lower()

    # äºˆæƒ³
    if "forecast" in cr or "nextaccumulated" in cr:
        return "äºˆæƒ³"

    # å‰æœŸ
    if cr.startswith("prior") or "prioryear" in cr or "prior1year" in cr or "prioraccumulated" in cr:
        if "instant" in cr:
            return "å‰æœŸæœ«"
        return "å‰æœŸ"

    # å‰å››åŠæœŸ
    if "priorquarter" in cr or "prior1quarter" in cr:
        return "å‰å››åŠæœŸ"

    # å½“æœŸ
    if cr.startswith("current") or "currentyear" in cr or "currentaccumulated" in cr:
        if "instant" in cr:
            return "å½“æœŸæœ«"
        return "å½“æœŸ"

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    if "prior" in cr:
        if "instant" in cr:
            return "å‰æœŸæœ«"
        return "å‰æœŸ"
    if "current" in cr:
        if "instant" in cr:
            return "å½“æœŸæœ«"
        return "å½“æœŸ"

    return context_ref


def parse_xbrl_content(content: bytes, filename: str):
    """
    XBRLã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’è§£æã—ã€è²¡å‹™ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    inline XBRL (iXBRL) ã¨é€šå¸¸ã® XBRL ã®ä¸¡æ–¹ã«å¯¾å¿œã€‚

    iXBRL ã®å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã¯ä»¥ä¸‹ã®ã‚¿ã‚°ã«æ ¼ç´ã•ã‚Œã¦ã„ã‚‹:
      <ix:nonFraction name="jppfs_cor:NetSales" contextRef="..." ...>123,456</ix:nonFraction>
      <ix:nonNumeric  name="jpdei_cor:FilerNameInJapaneseDEI" contextRef="...">ä¼šç¤¾å</ix:nonNumeric>

    Returns:
        list of dict: å„è¦ç´ ã®æƒ…å ±ï¼ˆelement, label_ja, value, contextç­‰ï¼‰
    """
    is_ixbrl = filename.lower().endswith(('.htm', '.html'))

    if is_ixbrl:
        return _parse_ixbrl(content, filename)
    else:
        return _parse_regular_xbrl(content, filename)


def _get_all_text(elem):
    """è¦ç´ å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå­è¦ç´ ã®ãƒ†ã‚­ã‚¹ãƒˆå«ã‚€ï¼‰ã‚’å–å¾—"""
    return ''.join(elem.itertext()).strip()


def _parse_ixbrl(content: bytes, filename: str):
    """
    inline XBRL (iXBRL) ã‚’è§£æã™ã‚‹ã€‚
    ix:nonFraction / ix:nonNumeric ã‚¿ã‚°ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã€‚

    æ³¨æ„: lxml HTMLParser ã¯å±æ€§åã‚’ã™ã¹ã¦å°æ–‡å­—ã«ã™ã‚‹ã€‚
      contextRef â†’ contextref, unitRef â†’ unitref ç­‰
    """
    try:
        parser = etree.HTMLParser(encoding='utf-8')
        tree = etree.fromstring(content, parser)
    except Exception as e:
        print(f"   âŒ iXBRLè§£æã‚¨ãƒ©ãƒ¼: {e}")
        return []

    # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚’å–å¾—ï¼ˆHTMLãƒ‘ãƒ¼ã‚µãƒ¼ç”¨: å±æ€§åå°æ–‡å­—å¯¾å¿œï¼‰
    contexts = parse_contexts(tree)

    results = []

    # ix:nonFraction / ix:nonNumeric ã‚’æ¢ç´¢
    # HTMLãƒ‘ãƒ¼ã‚µãƒ¼ã§ã¯åå‰ç©ºé–“ãªã—ã® "ix:nonfraction" / "ix:nonnumeric" ã¨ã—ã¦å‡ºç¾
    target_tags = {'ix:nonfraction', 'ix:nonnumeric'}

    for elem in tree.iter():
        tag = str(elem.tag).lower()

        if tag not in target_tags:
            continue

        # name å±æ€§ã‹ã‚‰è¦ç´ åã‚’å–å¾— (ä¾‹: "tse-ed-t:SalesIFRS", "jppfs_cor:NetSales")
        name_attr = elem.get("name", "")
        if not name_attr:
            continue

        # contextRef â†’ HTMLãƒ‘ãƒ¼ã‚µãƒ¼ã§å°æ–‡å­—åŒ–ã•ã‚Œã¦ contextref
        context_ref = elem.get("contextref", "")
        if not context_ref:
            continue

        # è¦ç´ åã‚’åˆ†è§£
        if ":" in name_attr:
            ns_prefix, element_name = name_attr.split(":", 1)
        else:
            ns_prefix = ""
            element_name = name_attr

        # ãƒ†ã‚­ã‚¹ãƒˆå€¤ã‚’å–å¾—ï¼ˆå­è¦ç´ å†…ãƒ†ã‚­ã‚¹ãƒˆã‚‚å«ã‚€ï¼‰
        text = _get_all_text(elem)

        # signå±æ€§ï¼ˆHTMLãƒ‘ãƒ¼ã‚µãƒ¼ã¯å°æ–‡å­—åŒ–ã™ã‚‹ï¼‰
        sign = elem.get("sign", "")
        # formatå±æ€§
        fmt = elem.get("format", "")
        # scaleå±æ€§ï¼ˆæ¡ã‚¹ã‚±ãƒ¼ãƒ«: ä¾‹ scale="6" â†’ ç™¾ä¸‡å˜ä½ã§è¡¨ç¤ºã•ã‚ŒãŸæ•°å€¤ã‚’å††ã«å¤‰æ›ï¼‰
        scale = elem.get("scale", "0")
        # unitrefï¼ˆå°æ–‡å­—åŒ–ï¼‰
        unit_ref = elem.get("unitref", "")
        # decimals
        decimals = elem.get("decimals", "")

        if not text:
            continue

        # æ•°å€¤ãƒ‘ãƒ¼ã‚¹ï¼ˆix:nonfraction ã®å ´åˆï¼‰
        value = None
        if tag == 'ix:nonfraction':
            try:
                clean = text.replace(",", "").replace("ï¼Œ", "").replace(" ", "").replace("\u3000", "")
                clean = clean.replace("â–³", "-").replace("â–²", "-")
                if clean.startswith("(") and clean.endswith(")"):
                    clean = "-" + clean[1:-1]
                # ãƒã‚¤ãƒ•ãƒ³ç³»ï¼ˆè©²å½“ãªã—ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                if clean in ("-", "ï¼", "â€•", "â€”", ""):
                    continue
                value = float(clean)
                # signå±æ€§
                if sign == "-":
                    value = -abs(value)
                # scaleå±æ€§ï¼ˆã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ï¼‰
                try:
                    sc = int(scale)
                    if sc != 0:
                        value = value * (10 ** sc)
                except ValueError:
                    pass
            except (ValueError, TypeError):
                pass

        # æœŸé–“ã‚¿ã‚¤ãƒ—ã®åˆ¤å®š
        period_type = classify_period(context_ref, contexts)

        # TDnetã‚µãƒãƒªãƒ¼è¦ç´ åãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆtse-ed-tç‹¬è‡ªå â†’ æ¨™æº–åï¼‰
        mapped_name = TSE_ELEMENT_MAP.get(element_name, element_name)
        label_ja = XBRL_LABEL_MAP.get(mapped_name, XBRL_LABEL_MAP.get(element_name, ""))

        results.append({
            "element": mapped_name,
            "label_ja": label_ja,
            "namespace": ns_prefix,
            "context_ref": context_ref,
            "period_type": period_type,
            "value": value,
            "value_raw": text,
            "unit_ref": unit_ref,
            "decimals": decimals,
        })

    return results


def _parse_regular_xbrl(content: bytes, filename: str):
    """é€šå¸¸ã® XBRL ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è§£æã™ã‚‹ã€‚"""
    try:
        tree = etree.fromstring(content)
    except etree.XMLSyntaxError:
        try:
            parser = etree.HTMLParser()
            tree = etree.fromstring(content, parser)
        except Exception as e:
            print(f"   âŒ XBRLè§£æã‚¨ãƒ©ãƒ¼: {e}")
            return []

    contexts = parse_contexts(tree)
    results = []

    for elem in tree.iter():
        tag = str(elem.tag)
        if '}' not in tag:
            continue

        local_name = etree.QName(tag).localname
        namespace = etree.QName(tag).namespace or ""

        context_ref = elem.get("contextRef")
        if context_ref is None:
            continue

        text = elem.text
        if text is None or text.strip() == "":
            continue
        text = text.strip()

        value = None
        try:
            clean = text.replace(",", "").replace("ï¼Œ", "")
            if clean.startswith("(") and clean.endswith(")"):
                clean = "-" + clean[1:-1]
            clean = clean.replace("â–³", "-").replace("â–²", "-")
            value = float(clean)
        except (ValueError, TypeError):
            pass

        period_type = classify_period(context_ref, contexts)
        label_ja = XBRL_LABEL_MAP.get(local_name, "")

        ns_short = ""
        if namespace:
            if "jppfs" in namespace:
                ns_short = "jppfs_cor"
            elif "jpdei" in namespace:
                ns_short = "jpdei_cor"
            elif "jpcrp" in namespace:
                ns_short = "jpcrp_cor"
            elif "jpigp" in namespace:
                ns_short = "jpigp_cor"
            else:
                parts = namespace.rstrip("/").split("/")
                ns_short = parts[-1] if parts else namespace

        results.append({
            "element": local_name,
            "label_ja": label_ja,
            "namespace": ns_short,
            "context_ref": context_ref,
            "period_type": period_type,
            "value": value,
            "value_raw": text,
            "unit_ref": elem.get("unitRef", ""),
            "decimals": elem.get("decimals", ""),
        })

    return results


# ============================================================
# Section 3: DataFrameæ§‹ç¯‰
# ============================================================

def build_dataframe(parsed_data: list) -> pd.DataFrame:
    """è§£æçµæœã‚’DataFrameã«å¤‰æ›"""
    df = pd.DataFrame(parsed_data)

    if df.empty:
        return df

    # ãƒ©ãƒ™ãƒ«ãŒç©ºã®ã‚‚ã®ã«ã‚‚è¦ç´ åã‚’è¡¨ç¤º
    df["display_name"] = df.apply(
        lambda row: row["label_ja"] if row["label_ja"] else row["element"],
        axis=1
    )

    return df


def build_financial_summary(df: pd.DataFrame) -> pd.DataFrame:
    """
    å½“æœŸã¨å‰æœŸã®ãƒ‡ãƒ¼ã‚¿ã‚’æ¨ªä¸¦ã³ã«ã—ãŸè²¡å‹™ã‚µãƒãƒªãƒ¼ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚
    åŒä¸€è¦ç´ ã§å½“æœŸ/å‰æœŸã®å€¤ã‚’æ¯”è¼ƒã—ã€å¢—æ¸›é¡ãƒ»å¢—æ¸›ç‡ã‚’è¨ˆç®—ã€‚
    """
    if df.empty:
        return pd.DataFrame()

    numeric_df = df[df["value"].notna()].copy()
    if numeric_df.empty:
        return pd.DataFrame()

    summary_rows = []
    elements = numeric_df["element"].unique()

    for elem_name in elements:
        elem_data = numeric_df[numeric_df["element"] == elem_name]
        label = XBRL_LABEL_MAP.get(elem_name, elem_name)

        current_val = None
        prior_val = None

        for _, row in elem_data.iterrows():
            pt = row["period_type"]
            val = row["value"]

            if pt in ("å½“æœŸ", "å½“æœŸæœ«", "å½“å››åŠæœŸ"):
                if current_val is None:
                    current_val = val
            elif pt in ("å‰æœŸ", "å‰æœŸæœ«", "å‰å››åŠæœŸ"):
                if prior_val is None:
                    prior_val = val

        if current_val is not None or prior_val is not None:
            # å¢—æ¸›é¡ãƒ»å¢—æ¸›ç‡ã®è¨ˆç®—
            change = None
            change_rate = None

            if current_val is not None and prior_val is not None and prior_val != 0:
                change = current_val - prior_val
                change_rate = change / abs(prior_val)

            summary_rows.append({
                "è¦ç´ å": elem_name,
                "å‹˜å®šç§‘ç›®": label,
                "å½“æœŸ": current_val,
                "å‰æœŸ": prior_val,
                "å¢—æ¸›é¡": change,
                "å¢—æ¸›ç‡": change_rate,
            })

    return pd.DataFrame(summary_rows)


# ============================================================
# Section 4: è²¡å‹™åˆ†æ
# ============================================================

def analyze_significant_changes(summary_df: pd.DataFrame, threshold: float = 0.20) -> pd.DataFrame:
    """
    å¤§ããå¢—æ¸›å¤‰åŒ–ã—ãŸå‹˜å®šç§‘ç›®ã‚’æ¤œå‡ºã™ã‚‹ã€‚

    Args:
        summary_df: è²¡å‹™ã‚µãƒãƒªãƒ¼DataFrame
        threshold:  å¤‰å‹•ç‡ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ20%ï¼‰

    Returns:
        DataFrame: é–¾å€¤ä»¥ä¸Šã®å¤‰å‹•ãŒã‚ã£ãŸå‹˜å®šç§‘ç›®ï¼ˆå¤‰å‹•ç‡ã®çµ¶å¯¾å€¤é™é †ï¼‰
    """
    if summary_df.empty or "å¢—æ¸›ç‡" not in summary_df.columns:
        return pd.DataFrame()

    sig = summary_df[summary_df["å¢—æ¸›ç‡"].notna()].copy()
    sig = sig[sig["å¢—æ¸›ç‡"].abs() >= threshold]
    sig = sig.sort_values("å¢—æ¸›ç‡", ascending=False, key=abs)

    return sig


def calculate_profit_margins(summary_df: pd.DataFrame) -> pd.DataFrame:
    """
    å„ç¨®åˆ©ç›Šç‡ã‚’è¨ˆç®—ã™ã‚‹ã€‚

    è¨ˆç®—ã™ã‚‹æŒ‡æ¨™:
      - å£²ä¸Šç·åˆ©ç›Šç‡   = å£²ä¸Šç·åˆ©ç›Š / å£²ä¸Šé«˜
      - å–¶æ¥­åˆ©ç›Šç‡     = å–¶æ¥­åˆ©ç›Š / å£²ä¸Šé«˜
      - çµŒå¸¸åˆ©ç›Šç‡     = çµŒå¸¸åˆ©ç›Š / å£²ä¸Šé«˜
      - å½“æœŸç´”åˆ©ç›Šç‡   = å½“æœŸç´”åˆ©ç›Š / å£²ä¸Šé«˜
    """
    if summary_df.empty:
        return pd.DataFrame()

    # å£²ä¸Šé«˜ã‚’å–å¾—ï¼ˆNetSales â†’ Revenue â†’ OperatingRevenue1 ã®é †ã§æ¢ã™ï¼‰
    sales_current = None
    sales_prior = None

    for sales_elem in ["NetSales", "Revenue", "OperatingRevenue1"]:
        row = summary_df[summary_df["è¦ç´ å"] == sales_elem]
        if not row.empty and row.iloc[0]["å½“æœŸ"] is not None:
            sales_current = row.iloc[0]["å½“æœŸ"]
            sales_prior = row.iloc[0]["å‰æœŸ"]
            break

    if sales_current is None or sales_current == 0:
        return pd.DataFrame()

    margin_items = [
        ("å£²ä¸Šç·åˆ©ç›Šç‡", "GrossProfit"),
        ("å–¶æ¥­åˆ©ç›Šç‡", "OperatingIncome"),
        ("çµŒå¸¸åˆ©ç›Šç‡", "OrdinaryIncome"),
        ("å½“æœŸç´”åˆ©ç›Šç‡", "ProfitLoss"),
        ("è¦ªä¼šç¤¾å¸°å±ç´”åˆ©ç›Šç‡", "ProfitLossAttributableToOwnersOfParent"),
    ]

    margin_rows = []
    for margin_name, elem_name in margin_items:
        row = summary_df[summary_df["è¦ç´ å"] == elem_name]
        if row.empty:
            continue

        curr = row.iloc[0]["å½“æœŸ"]
        prev = row.iloc[0]["å‰æœŸ"]

        curr_margin = (curr / sales_current * 100) if curr is not None else None
        prev_margin = (prev / sales_prior * 100) if prev is not None and sales_prior and sales_prior != 0 else None

        diff = None
        if curr_margin is not None and prev_margin is not None:
            diff = curr_margin - prev_margin

        margin_rows.append({
            "æŒ‡æ¨™": margin_name,
            "å½“æœŸï¼ˆ%ï¼‰": round(curr_margin, 2) if curr_margin is not None else None,
            "å‰æœŸï¼ˆ%ï¼‰": round(prev_margin, 2) if prev_margin is not None else None,
            "å·®åˆ†ï¼ˆptï¼‰": round(diff, 2) if diff is not None else None,
        })

    return pd.DataFrame(margin_rows)


# ============================================================
# Section 5: Excelå‡ºåŠ›
# ============================================================

def export_to_excel(
    company_info: dict,
    summary_df: pd.DataFrame,
    significant_df: pd.DataFrame,
    margins_df: pd.DataFrame,
    raw_df: pd.DataFrame,
    output_path: str,
):
    """åˆ†æçµæœã‚’æ›¸å¼ä»˜ãExcelãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›"""

    wb = Workbook()

    # ã‚¹ã‚¿ã‚¤ãƒ«å®šç¾©
    header_font = Font(bold=True, size=11, color="FFFFFF")
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    alert_fill = PatternFill(start_color="FFE0E0", end_color="FFE0E0", fill_type="solid")
    warn_fill = PatternFill(start_color="FFFFD0", end_color="FFFFD0", fill_type="solid")
    good_fill = PatternFill(start_color="E0FFE0", end_color="E0FFE0", fill_type="solid")
    number_fmt = '#,##0'
    pct_fmt = '0.0%'
    thin_border = Border(
        left=Side(style='thin'), right=Side(style='thin'),
        top=Side(style='thin'), bottom=Side(style='thin')
    )

    def style_header_row(ws, row_num, num_cols):
        """ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é©ç”¨"""
        for col in range(1, num_cols + 1):
            cell = ws.cell(row=row_num, column=col)
            cell.font = header_font
            cell.fill = header_fill
            cell.alignment = Alignment(horizontal='center')
            cell.border = thin_border

    def auto_column_width(ws):
        """åˆ—å¹…ã‚’è‡ªå‹•èª¿æ•´"""
        for col_cells in ws.columns:
            max_length = 0
            col_letter = get_column_letter(col_cells[0].column)
            for cell in col_cells:
                if cell.value:
                    text = str(cell.value)
                    length = len(text)
                    # æ—¥æœ¬èªæ–‡å­—ã¯å¹…2å€æ‰±ã„
                    for c in text:
                        if ord(c) > 127:
                            length += 1
                    max_length = max(max_length, length)
            ws.column_dimensions[col_letter].width = min(max_length + 4, 50)

    # ===================================================
    # Sheet 1: åˆ†æã‚µãƒãƒªãƒ¼
    # ===================================================
    ws1 = wb.active
    ws1.title = "åˆ†æã‚µãƒãƒªãƒ¼"

    # ä¼šç¤¾æƒ…å ±ãƒ˜ãƒƒãƒ€ãƒ¼
    info_items = [
        ("ä¼šç¤¾å", company_info.get("name", "")),
        ("ã‚³ãƒ¼ãƒ‰", company_info.get("code", "")),
        ("è¡¨é¡Œ", company_info.get("title", "")),
        ("æ—¥ä»˜", company_info.get("date", "")),
    ]
    for i, (key, val) in enumerate(info_items, 1):
        ws1.cell(row=i, column=1, value=key).font = Font(bold=True)
        ws1.cell(row=i, column=2, value=val)

    current_row = len(info_items) + 2

    # --- åˆ©ç›Šç‡ãƒ†ãƒ¼ãƒ–ãƒ« ---
    if not margins_df.empty:
        ws1.cell(row=current_row, column=1, value="ã€åˆ©ç›Šç‡åˆ†æã€‘").font = Font(bold=True, size=12)
        current_row += 1

        for c_idx, col_name in enumerate(margins_df.columns, 1):
            ws1.cell(row=current_row, column=c_idx, value=col_name)
        style_header_row(ws1, current_row, len(margins_df.columns))
        current_row += 1

        for _, row in margins_df.iterrows():
            for c_idx, col_name in enumerate(margins_df.columns, 1):
                cell = ws1.cell(row=current_row, column=c_idx, value=row[col_name])
                cell.border = thin_border
                if col_name == "å·®åˆ†ï¼ˆptï¼‰" and row[col_name] is not None:
                    if row[col_name] > 0:
                        cell.fill = good_fill
                    elif row[col_name] < -1:
                        cell.fill = alert_fill
            current_row += 1

        current_row += 1

    # --- å¤§å¹…å¤‰å‹•ãƒ†ãƒ¼ãƒ–ãƒ« ---
    if not significant_df.empty:
        ws1.cell(row=current_row, column=1, value="ã€å¤§å¹…å¤‰å‹•ã®å‹˜å®šç§‘ç›®ã€‘").font = Font(bold=True, size=12)
        current_row += 1

        display_cols = ["å‹˜å®šç§‘ç›®", "å½“æœŸ", "å‰æœŸ", "å¢—æ¸›é¡", "å¢—æ¸›ç‡"]
        avail_cols = [c for c in display_cols if c in significant_df.columns]

        for c_idx, col_name in enumerate(avail_cols, 1):
            ws1.cell(row=current_row, column=c_idx, value=col_name)
        style_header_row(ws1, current_row, len(avail_cols))
        current_row += 1

        for _, row in significant_df.iterrows():
            for c_idx, col_name in enumerate(avail_cols, 1):
                val = row[col_name]
                cell = ws1.cell(row=current_row, column=c_idx, value=val)
                cell.border = thin_border

                if col_name in ("å½“æœŸ", "å‰æœŸ", "å¢—æ¸›é¡") and isinstance(val, (int, float)):
                    cell.number_format = number_fmt
                elif col_name == "å¢—æ¸›ç‡" and isinstance(val, (int, float)):
                    cell.number_format = pct_fmt
                    if abs(val) >= 0.5:
                        cell.fill = alert_fill
                    elif abs(val) >= 0.3:
                        cell.fill = warn_fill
            current_row += 1

    auto_column_width(ws1)

    # ===================================================
    # Sheet 2: è²¡å‹™ãƒ‡ãƒ¼ã‚¿ä¸€è¦§ï¼ˆå½“æœŸ/å‰æœŸæ¯”è¼ƒï¼‰
    # ===================================================
    if not summary_df.empty:
        ws2 = wb.create_sheet("è²¡å‹™ãƒ‡ãƒ¼ã‚¿ä¸€è¦§")

        display_cols = ["å‹˜å®šç§‘ç›®", "å½“æœŸ", "å‰æœŸ", "å¢—æ¸›é¡", "å¢—æ¸›ç‡"]
        avail_cols = [c for c in display_cols if c in summary_df.columns]

        for c_idx, col_name in enumerate(avail_cols, 1):
            ws2.cell(row=1, column=c_idx, value=col_name)
        style_header_row(ws2, 1, len(avail_cols))

        for r_idx, (_, row) in enumerate(summary_df.iterrows(), 2):
            for c_idx, col_name in enumerate(avail_cols, 1):
                val = row[col_name]
                cell = ws2.cell(row=r_idx, column=c_idx, value=val)
                cell.border = thin_border

                if col_name in ("å½“æœŸ", "å‰æœŸ", "å¢—æ¸›é¡") and isinstance(val, (int, float)):
                    cell.number_format = number_fmt
                elif col_name == "å¢—æ¸›ç‡" and isinstance(val, (int, float)):
                    cell.number_format = pct_fmt
                    if abs(val) >= 0.3:
                        cell.fill = alert_fill
                    elif abs(val) >= 0.2:
                        cell.fill = warn_fill

        auto_column_width(ws2)

    # ===================================================
    # Sheet 3: XBRLãƒ‡ãƒ¼ã‚¿ï¼ˆRawï¼‰
    # ===================================================
    if not raw_df.empty:
        ws3 = wb.create_sheet("XBRLãƒ‡ãƒ¼ã‚¿ï¼ˆRawï¼‰")

        raw_display_cols = ["display_name", "element", "namespace", "period_type",
                            "value", "value_raw", "unit_ref", "context_ref"]
        avail_cols = [c for c in raw_display_cols if c in raw_df.columns]
        header_names = {
            "display_name": "å‹˜å®šç§‘ç›®",
            "element": "XBRLè¦ç´ å",
            "namespace": "åå‰ç©ºé–“",
            "period_type": "æœŸé–“",
            "value": "æ•°å€¤",
            "value_raw": "åŸæ–‡",
            "unit_ref": "å˜ä½",
            "context_ref": "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ",
        }

        for c_idx, col_name in enumerate(avail_cols, 1):
            ws3.cell(row=1, column=c_idx, value=header_names.get(col_name, col_name))
        style_header_row(ws3, 1, len(avail_cols))

        for r_idx, (_, row) in enumerate(raw_df.iterrows(), 2):
            for c_idx, col_name in enumerate(avail_cols, 1):
                val = row[col_name]
                cell = ws3.cell(row=r_idx, column=c_idx, value=val)
                cell.border = thin_border
                if col_name == "value" and isinstance(val, (int, float)):
                    cell.number_format = number_fmt

        auto_column_width(ws3)

    # ä¿å­˜
    wb.save(output_path)
    print(f"   ğŸ“Š Excelå‡ºåŠ›: {output_path}")


# ============================================================
# å¼•æ•°
# ============================================================

def parse_args():
    p = argparse.ArgumentParser(
        description="â‘¢ TDnet XBRLå–å¾— & è²¡å‹™åˆ†æãƒ„ãƒ¼ãƒ«"
    )
    p.add_argument("--target", default=DEFAULT_TARGET_SPEC,
                    help="YYYYMMDD / YYYYMM / 'YYYYMMDD YYYYMMDD'")
    p.add_argument("--code", default=None,
                    help="è¨¼åˆ¸ã‚³ãƒ¼ãƒ‰ã§ãƒ•ã‚£ãƒ«ã‚¿ï¼ˆä¾‹: 7203ï¼‰")
    p.add_argument("--save-root", default=DEFAULT_SAVE_ROOT,
                    help="ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€")
    p.add_argument("--threshold", type=float, default=DEFAULT_CHANGE_THRESHOLD,
                    help="å¤§å¹…å¤‰å‹•ã®é–¾å€¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ0.20=20%%ï¼‰")
    p.add_argument("--page-sleep", type=float, default=PAGE_SLEEP_SEC)
    p.add_argument("--xbrl-sleep", type=float, default=XBRL_SLEEP_SEC)
    return p.parse_args()


# ============================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ============================================================

def process_single_xbrl(zip_path, company_info, threshold, output_dir):
    """1ã¤ã®XBRL ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’è§£æãƒ»åˆ†æãƒ»Excelå‡ºåŠ›ã™ã‚‹"""

    # XBRL ZIPã‹ã‚‰ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¢ã™
    filename, content = find_xbrl_instance_in_zip(zip_path)
    if content is None:
        print(f"   âš ï¸ XBRLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {zip_path}")
        return

    print(f"   ğŸ“„ XBRLã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: {filename}")

    # ãƒ‘ãƒ¼ã‚¹
    parsed_data = parse_xbrl_content(content, filename)
    if not parsed_data:
        print("   âš ï¸ è²¡å‹™ãƒ‡ãƒ¼ã‚¿ãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return

    print(f"   ğŸ“Š æŠ½å‡ºè¦ç´ æ•°: {len(parsed_data)}")

    # DataFrameæ§‹ç¯‰
    raw_df = build_dataframe(parsed_data)
    summary_df = build_financial_summary(raw_df)

    # åˆ†æ
    significant_df = analyze_significant_changes(summary_df, threshold)
    margins_df = calculate_profit_margins(summary_df)

    # ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«çµæœè¡¨ç¤º
    if not margins_df.empty:
        print("\n   ğŸ“ˆ ã€åˆ©ç›Šç‡ã€‘")
        for _, row in margins_df.iterrows():
            curr = f"{row['å½“æœŸï¼ˆ%ï¼‰']:.1f}%" if row['å½“æœŸï¼ˆ%ï¼‰'] is not None else "N/A"
            prev = f"{row['å‰æœŸï¼ˆ%ï¼‰']:.1f}%" if row['å‰æœŸï¼ˆ%ï¼‰'] is not None else "N/A"
            print(f"      {row['æŒ‡æ¨™']}: å½“æœŸ {curr} â† å‰æœŸ {prev}")

    if not significant_df.empty:
        print(f"\n   âš ï¸ ã€å¤§å¹…å¤‰å‹•ï¼ˆé–¾å€¤{threshold:.0%}ä»¥ä¸Šï¼‰ã€‘")
        for _, row in significant_df.head(10).iterrows():
            label = row["å‹˜å®šç§‘ç›®"] if row["å‹˜å®šç§‘ç›®"] else row["è¦ç´ å"]
            rate = row["å¢—æ¸›ç‡"]
            direction = "â†‘" if rate > 0 else "â†“"
            print(f"      {direction} {label}: {rate:+.1%}")

    # Excelå‡ºåŠ›
    code = company_info.get("code", "unknown")
    name = safe_filename(company_info.get("name", "unknown"), max_len=20)
    excel_name = f"XBRLåˆ†æ_{code}_{name}.xlsx"
    excel_path = output_dir / excel_name

    export_to_excel(company_info, summary_df, significant_df, margins_df, raw_df, str(excel_path))


def main():
    args = parse_args()

    global PAGE_SLEEP_SEC, XBRL_SLEEP_SEC
    PAGE_SLEEP_SEC = args.page_sleep
    XBRL_SLEEP_SEC = args.xbrl_sleep

    save_root = Path(args.save_root)
    save_root.mkdir(parents=True, exist_ok=True)

    d_from, d_to, label, mode = parse_target_spec(args.target)

    print("=" * 60)
    print("â‘¢ XBRL Financial Analyzer")
    print("=" * 60)
    print(f"ğŸ¯ å¯¾è±¡æœŸé–“: {d_from} ï½ {d_to} (mode={mode})")
    if args.code:
        print(f"ğŸ” ã‚³ãƒ¼ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿: {args.code}")
    print(f"ğŸ“ ä¿å­˜å…ˆ: {save_root}")
    print(f"ğŸ“Š å¤‰å‹•é–¾å€¤: {args.threshold:.0%}")

    session = requests.Session()
    total_xbrl = 0
    total_analyzed = 0

    for target_date_str in iter_dates_yyyymmdd(d_from, d_to):
        print(f"\n{'=' * 60}")
        print(f"ğŸ“… æ—¥ä»˜: {target_date_str}")

        day_dir = save_root / target_date_str
        day_dir.mkdir(parents=True, exist_ok=True)

        # TDnetã‹ã‚‰XBRLãƒªãƒ³ã‚¯ã‚’å–å¾—
        xbrl_entries = find_xbrl_links(session, target_date_str, args.code)

        if not xbrl_entries:
            print("   ğŸ“ XBRLãƒ‡ãƒ¼ã‚¿ãªã—")
            continue

        print(f"   ğŸ“¦ XBRLå¯¾è±¡: {len(xbrl_entries)} ä»¶")

        for entry in xbrl_entries:
            code = entry["code"]
            name = entry["name"]
            title = entry["title"]

            print(f"\n   --- {code} {name} ---")
            print(f"   ğŸ“„ {title}")

            # ZIPãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
            zip_name = f"{safe_filename(code, 4)}_{safe_filename(name, 20)}_xbrl.zip"
            zip_path = day_dir / zip_name

            if zip_path.exists():
                print("   â­ï¸ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚ã‚Šï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            else:
                ok = download_xbrl_zip(session, entry["xbrl_url"], str(zip_path))
                if not ok:
                    continue
                print(f"   âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {zip_name}")
                total_xbrl += 1

                if XBRL_SLEEP_SEC > 0:
                    time.sleep(XBRL_SLEEP_SEC)

            # è§£æãƒ»åˆ†æ
            try:
                company_info = {
                    "code": code,
                    "name": name,
                    "title": title,
                    "date": target_date_str,
                }
                process_single_xbrl(zip_path, company_info, args.threshold, day_dir)
                total_analyzed += 1
            except Exception as e:
                print(f"   âŒ è§£æã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                traceback.print_exc()

    print(f"\n{'=' * 60}")
    print("âœ… â‘¢å®Œäº†")
    print(f"   XBRLãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰: {total_xbrl} ä»¶")
    print(f"   åˆ†æå®Œäº†: {total_analyzed} ä»¶")
    print(f"   ä¿å­˜å…ˆ: {save_root}")


if __name__ == "__main__":
    main()
