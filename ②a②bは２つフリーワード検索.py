# -*- coding: utf-8 -*-
"""
â‘¡ ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆâ‘¡A:PDFå…¨æ–‡æ¤œç´¢ / â‘¡B:é…å¸ƒç”¨CSVä½œæˆï¼‰

ã€Windowsãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œç‰ˆã€‘
- Colabå°‚ç”¨ã® `!pip` / `google.colab` / `/content/drive` ä¾å­˜ã‚’é™¤å»
- ä¿å­˜å…ˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«/ãƒ‰ãƒ©ã‚¤ãƒ–ãƒ‘ã‚¹ã«å¯¾å¿œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\TDnet_Downloadsï¼‰

ä½¿ã„æ–¹ï¼ˆã‚³ãƒ”ãƒšç”¨ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰:
  â‘¡Aï¼ˆå…¨æ–‡æ¤œç´¢: PDFæœ¬æ–‡ã«å¯¾ã™ã‚‹ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼‰:
    
python "â‘¡aâ‘¡bã¯ï¼’ã¤ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py" analyze --target "20260204" --keywords "ä¾¡æ ¼äº¤æ¸‰" "å¢—ç”£" "ä¾¡æ ¼æ”¹å®š" "ä¾¡æ ¼è»¢å«" "å€¤ä¸Š" "æƒ³å®šä»¥ä¸Š" "ä¸Šæ–¹ä¿®æ­£" "ä¸‹æ–¹ä¿®æ­£" "æƒ³å®šä»¥ä¸‹" "æœªé”" "å¤§å¹…" "è¨ˆç”»ã‚’ä¸Š" "è¨ˆç”»ã‚’ä¸‹" "è¨ˆç”»ä»¥" "éœ€è¦å›å¾©" "éœ€è¦ã®å›å¾©" "éœ€è¦ãŒå¢—" "éœ€è¦ãŒä½" "æ‚ªåŒ–" "ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ‹ãƒƒãƒãƒˆãƒƒãƒ—" "ãƒˆãƒƒãƒ—ã‚·ã‚§ã‚¢" "ã‚·ã‚§ ã‚¢æ‹¡å¤§" "ãƒ¬ã‚¢ã‚¢ãƒ¼ã‚¹"
  â‘¡Bï¼ˆé…å¸ƒç”¨CSVä½œæˆ: â‘ ã®CSVã¨â‘¡Açµæœã‚’çªåˆï¼‰:
    - é…å¸ƒç”¨ï¼ˆæ¨™æº–ãƒ»é…å¸ƒå…ˆãƒ•ã‚©ãƒ«ãƒ€å‘ã‘ / TDnetãƒªãƒ³ã‚¯ç‰ˆï¼‰:
        python "â‘¡aâ‘¡bã¯ï¼’ã¤ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py" distribute --target "20260204"

    - è‡ªåˆ†ç”¨ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«PDFã¸ã®ãƒªãƒ³ã‚¯ç‰ˆ / ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒ•ã‚©ãƒ«ãƒ€æŒ‡å®šï¼‰:
        python "â‘¡aâ‘¡bã¯ï¼’ã¤ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py" distribute --target "20260204" --save-root "G:\\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\\TDnet_Downloads" --local-link

    â€» --local-link ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšã€å‡ºåŠ›CSVã«ã¯ä»¥ä¸‹ã®3åˆ—ãŒå¸¸ã«å«ã¾ã‚Œã¾ã™:
      - ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã€: --local-linkæŒ‡å®šæ™‚ã¯ãƒ­ãƒ¼ã‚«ãƒ«ã€ãªã‘ã‚Œã°TDnet
      - ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_TDnetï¼‰ã€: å¸¸ã«TDnetã¸ã®ãƒªãƒ³ã‚¯
      - ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰ã€: å¸¸ã«ãƒ­ãƒ¼ã‚«ãƒ«PDFã¸ã®ãƒªãƒ³ã‚¯
    ã¤ã¾ã‚Šã€ã©ã¡ã‚‰ã®ã‚³ãƒãƒ³ãƒ‰ã§ã‚‚ä¸¡æ–¹ã®ãƒªãƒ³ã‚¯ãŒå«ã¾ã‚ŒãŸCSVãŒå‡ºåŠ›ã•ã‚Œã¾ã™ã€‚

  è¡¨é¡Œæ¤œç´¢ï¼ˆâ‘ ã®CSVã®ã€Œè¡¨é¡Œã€ã«å¯¾ã™ã‚‹éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ãƒ»é«˜é€Ÿï¼‰:
    ä¾‹ï¼‰è¡¨é¡Œã«ã€Œè³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡ã‚’æ„è­˜ã—ãŸã€ã‚’å«ã‚€ã‚‚ã®ã‚’ãƒªã‚¹ãƒˆåŒ–:
        python "â‘¡aâ‘¡bã¯ï¼’ã¤ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢.py" title --target "20260204" --keywords "è³‡æœ¬ã‚³ã‚¹ãƒˆã‚„æ ªä¾¡ã‚’æ„è­˜ã—ãŸ"
"""

import os
import re
import pandas as pd
import datetime
import unicodedata
import shutil
import argparse
from pathlib import Path

try:
    import fitz  # PyMuPDF
except Exception as e:
    fitz = None
    _FITZ_IMPORT_ERROR = e

DEFAULT_SAVE_ROOT = r"G:\ãƒã‚¤ãƒ‰ãƒ©ã‚¤ãƒ–\TDnet_Downloads"
DEFAULT_TARGET_SPEC = "20251212 20260202"
DEFAULT_SEARCH_KEYWORDS = ["ä¾¡æ ¼äº¤æ¸‰", "å¢—ç”£"]
PAGES_SEPARATOR = " "
ANALYSIS_CSV_PREFIX = "Analysis_Hits_free_word"
DISTRIBUTION_CSV_PREFIX = "PDF_Search_Result_Distribution_free_word"
TITLE_SEARCH_CSV_PREFIX = "Title_Hits_free_word"

# -----------------------------
# æ—¥ä»˜æŒ‡å®šå‡¦ç†
# -----------------------------
def parse_target_spec(spec: str):
    spec = spec.strip()
    parts = spec.split()

    if len(parts) == 1:
        s = parts[0]
        if re.fullmatch(r"\d{8}", s):
            return s, s, s, "day"
        if re.fullmatch(r"\d{6}", s):
            y = int(s[:4]); m = int(s[4:6])
            start = datetime.date(y, m, 1)
            if m == 12:
                end = datetime.date(y + 1, 1, 1) - datetime.timedelta(days=1)
            else:
                end = datetime.date(y, m + 1, 1) - datetime.timedelta(days=1)
            return start.strftime("%Y%m%d"), end.strftime("%Y%m%d"), s, "month"
        raise ValueError("TARGET_SPEC ã¯ 'YYYYMMDD' / 'YYYYMM' / 'YYYYMMDD YYYYMMDD' ã®ã„ãšã‚Œã‹ã§ã™ã€‚")

    if len(parts) == 2:
        d1, d2 = parts
        if not (re.fullmatch(r"\d{8}", d1) and re.fullmatch(r"\d{8}", d2)):
            raise ValueError("ç¯„å›²æŒ‡å®šã¯ 'YYYYMMDD YYYYMMDD' å½¢å¼ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        if d1 > d2:
            d1, d2 = d2, d1
        return d1, d2, f"{d1}_{d2}", "range"

    raise ValueError("TARGET_SPEC ã®æŒ‡å®šãŒä¸æ­£ã§ã™ã€‚")

def list_date_folders(root_path: str):
    if not os.path.isdir(root_path):
        return []
    return sorted([
        d for d in os.listdir(root_path)
        if os.path.isdir(os.path.join(root_path, d)) and re.fullmatch(r"\d{8}", d)
    ])

def select_target_folders(root_path: str, target_spec: str):
    d_from, d_to, label, mode = parse_target_spec(target_spec)
    targets = [d for d in list_date_folders(root_path) if d_from <= d <= d_to]
    return targets, label, (d_from, d_to), mode

# -----------------------------
# PDFè§£æï¼ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ»ãƒšãƒ¼ã‚¸æ•°ãƒ»ãƒšãƒ¼ã‚¸ç•ªå·ï¼‰
# -----------------------------
def extract_hits_pages_from_pdf(pdf_path: str, keywords, pages_sep=" "):
    """
    æˆ»ã‚Šå€¤:
      (ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ–‡å­—åˆ—, ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°, è¨˜è¿°ãƒšãƒ¼ã‚¸)
    """
    try:
        doc = fitz.open(pdf_path)

        hits = []
        hit_pages = set()

        for page_index, page in enumerate(doc, start=1):
            text = page.get_text("text")
            page_hit = False

            for kw in keywords:
                if kw in text:
                    page_hit = True
                    if kw not in hits:
                        hits.append(kw)

            if page_hit:
                hit_pages.add(page_index)

        doc.close()

        pages_sorted = sorted(hit_pages)
        pages_str = pages_sep.join(str(p) for p in pages_sorted)

        return " ".join(hits), len(pages_sorted), pages_str

    except Exception as e:
        print(f"è§£æå¤±æ•—: {pdf_path} / {e}")
        return "", 0, ""

# -----------------------------
# ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å‡¦ç†ç”¨
# -----------------------------

def archive_if_exists(path: str):
    if not os.path.exists(path):
        return

    base_dir = os.path.dirname(path)
    base_name = os.path.basename(path)

    archive_dir = os.path.join(base_dir, "archive")
    os.makedirs(archive_dir, exist_ok=True)

    ts = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    name, ext = os.path.splitext(base_name)

    archived_name = f"{name}_{ts}{ext}"
    archived_path = os.path.join(archive_dir, archived_name)

    shutil.move(path, archived_path)
    print(f"ğŸ—‚ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã¸é€€é¿: {archived_name}")

# -----------------------------
# ã‚³ãƒ¼ãƒ‰æŠ½å‡ºï¼ˆâ‘ ã®å‘½åè¦å‰‡å‰æï¼‰
# -----------------------------
def extract_code_from_pdf_filename(pdf_filename: str) -> str:
    m = re.match(r"^([0-9A-Za-z]{4})_", str(pdf_filename))
    return m.group(1).upper() if m else ""

REQUIRED_META_FIELDS = ["åˆ†é¡", "æ™‚åˆ»", "ã‚³ãƒ¼ãƒ‰", "ä¼šç¤¾å", "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "URLï¼ˆç”Ÿï¼‰"]


def norm_key(s: str) -> str:
    return unicodedata.normalize("NFKC", str(s)).strip()


def find_empty_fields(meta: dict, required_fields):
    return [k for k in required_fields if not (meta.get(k, "") or "").strip()]


def build_tdnet_index_for_dates(root_path: str, dates):
    """
    â‘ ã®CSVã‚’èª­ã¿è¾¼ã‚“ã§çªåˆç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½œæˆ
    key = (æ—¥ä»˜, æ­£è¦åŒ–PDFãƒ•ã‚¡ã‚¤ãƒ«å)
    """
    index = {}
    missing_csv_dates = []

    for d in sorted(set(dates)):
        day_csv = os.path.join(root_path, d, f"TDnet_Sorted_{d}.csv")
        root_csv = os.path.join(root_path, f"TDnet_Sorted_{d}.csv")
        csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None

        if csv_path is None:
            missing_csv_dates.append(d)
            continue

        df = pd.read_csv(csv_path, dtype=str).fillna("")
        df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]

        if "PDFãƒ•ã‚¡ã‚¤ãƒ«å" not in df.columns:
            raise ValueError(f"{csv_path} ã« PDFãƒ•ã‚¡ã‚¤ãƒ«å åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")

        for _, r in df.iterrows():
            pdf_raw = r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", "")
            pdf_key = norm_key(pdf_raw)
            if not pdf_key:
                continue

            index[(d, pdf_key)] = {
                "åˆ†é¡": r.get("åˆ†é¡", "").strip(),
                "æ™‚åˆ»": r.get("æ™‚åˆ»", "").strip(),
                "ã‚³ãƒ¼ãƒ‰": (r.get("ã‚³ãƒ¼ãƒ‰", "").strip()[:4]),
                "ä¼šç¤¾å": r.get("ä¼šç¤¾å", "").strip(),
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰": r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "").strip(),
                "URLï¼ˆç”Ÿï¼‰": r.get("URLï¼ˆç”Ÿï¼‰", "").strip(),
            }

    return index, missing_csv_dates


def run_title_search(root_dir: str, target_spec: str, keywords):
    """
    â‘ ã®CSVï¼ˆTDnet_Sorted_YYYYMMDD.csvï¼‰ã®ã€Œè¡¨é¡Œã€ã¾ãŸã¯ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã€ã«å¯¾ã—ã¦
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’éƒ¨åˆ†ä¸€è‡´æ¤œç´¢ã—ã€ãƒ’ãƒƒãƒˆã—ãŸæ˜ç´°ã‚’CSVã«å‡ºåŠ›ã™ã‚‹ã€‚
    PDFæœ¬æ–‡ã¯èª­ã¾ãªã„ãŸã‚ã€é«˜é€Ÿã€‚
    """
    targets, label, (d_from, d_to), mode = select_target_folders(root_dir, target_spec)

    print("ãƒ«ãƒ¼ãƒˆ:", root_dir)
    print("å¯¾è±¡æŒ‡å®š:", target_spec, "mode=", mode, "from=", d_from, "to=", d_to)
    print("å¯¾è±¡æ—¥æ•°:", len(targets))
    print("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¡¨é¡Œç”¨ï¼‰:", list(keywords))

    if not targets:
        raise FileNotFoundError("å¯¾è±¡æœŸé–“ã«è©²å½“ã™ã‚‹æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    hits = []
    total_rows = 0
    hit_rows = 0

    for idx, d in enumerate(sorted(targets), start=1):
        day_csv = os.path.join(root_dir, d, f"TDnet_Sorted_{d}.csv")
        root_csv = os.path.join(root_dir, f"TDnet_Sorted_{d}.csv")
        csv_path = day_csv if os.path.exists(day_csv) else root_csv if os.path.exists(root_csv) else None

        if csv_path is None:
            print(f"âš  TDnet_Sorted_{d}.csv ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        print(f"[{idx}/{len(targets)}] æ—¥ä»˜ {d} ã®CSVã‚’å‡¦ç†ä¸­... ({csv_path})")

        df = pd.read_csv(csv_path, dtype=str).fillna("")
        df.columns = [str(c).strip().replace("\ufeff", "") for c in df.columns]

        # è¡¨é¡Œãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—ï¼ˆã‚ã‚Œã°ã€Œè¡¨é¡Œã€ã€ãªã‘ã‚Œã°ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã€ã‹ã‚‰è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼‰
        has_plain_title = "è¡¨é¡Œ" in df.columns
        has_link_title = "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰" in df.columns

        if not (has_plain_title or has_link_title):
            print(f"âš  {csv_path} ã« è¡¨é¡Œ / è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
            continue

        for _, r in df.iterrows():
            total_rows += 1

            # è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã®å…ƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            title_link_tdnet = str(r.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")).strip()

            if has_plain_title:
                title_text = str(r.get("è¡¨é¡Œ", "")).strip()
            else:
                raw = title_link_tdnet
                # =HYPERLINK("URL","è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ") å½¢å¼ã‹ã‚‰è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã ã‘æŠ½å‡º
                m = re.match(r'=HYPERLINK\(".*?",\s*"([^"]*)"\)', raw)
                title_text = m.group(1) if m else raw

            if not title_text:
                continue

            # ã„ãšã‚Œã‹ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒéƒ¨åˆ†ä¸€è‡´ã™ã‚Œã°ãƒ’ãƒƒãƒˆ
            if not any(kw in title_text for kw in keywords):
                continue

            hit_rows += 1

            # ãƒ­ãƒ¼ã‚«ãƒ«PDFã¸ã®ãƒªãƒ³ã‚¯ã‚’ç”Ÿæˆ
            pdf_filename = str(r.get("PDFãƒ•ã‚¡ã‚¤ãƒ«å", "")).strip()
            display_text = title_text or pdf_filename
            local_pdf_path = os.path.join(root_dir, d, pdf_filename)
            title_link_local = f'=HYPERLINK("{local_pdf_path}", "{display_text}")'

            hits.append(
                {
                    "æ—¥ä»˜": d,
                    "æ™‚åˆ»": str(r.get("æ™‚åˆ»", "")).strip(),
                    "ã‚³ãƒ¼ãƒ‰": str(r.get("ã‚³ãƒ¼ãƒ‰", "")).strip()[:4],
                    "ä¼šç¤¾å": str(r.get("ä¼šç¤¾å", "")).strip(),
                    "è¡¨é¡Œ": title_text,
                    "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_TDnetï¼‰": title_link_tdnet,
                    "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰": title_link_local,
                    "åˆ†é¡": str(r.get("åˆ†é¡", "")).strip(),
                    "PDFãƒ•ã‚¡ã‚¤ãƒ«å": pdf_filename,
                    "URLï¼ˆç”Ÿï¼‰": str(r.get("URLï¼ˆç”Ÿï¼‰", "")).strip(),
                }
            )

    out_csv = f"{TITLE_SEARCH_CSV_PREFIX}_{label}.csv"
    out_path = os.path.join(root_dir, out_csv)

    if not hits:
        print("è¡¨é¡Œã«ãƒ’ãƒƒãƒˆã™ã‚‹è¡Œã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        out_df = pd.DataFrame(hits)
        out_df = out_df[
            [
                "æ—¥ä»˜",
                "æ™‚åˆ»",
                "ã‚³ãƒ¼ãƒ‰",
                "ä¼šç¤¾å",
                "è¡¨é¡Œ",
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_TDnetï¼‰",
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰",
                "åˆ†é¡",
                "PDFãƒ•ã‚¡ã‚¤ãƒ«å",
                "URLï¼ˆç”Ÿï¼‰",
            ]
        ]
        archive_if_exists(out_path)
        out_df.to_csv(out_path, index=False, encoding="utf-8-sig")

    print("\nâœ… è¡¨é¡Œæ¤œç´¢ å®Œäº†")
    print("ç·è¡Œæ•°:", total_rows)
    print("ãƒ’ãƒƒãƒˆè¡Œæ•°:", hit_rows)
    print("å‡ºåŠ›CSV:", out_csv)
    print("ä¿å­˜å…ˆ:", root_dir)


def run_analyze(root_dir: str, target_spec: str, keywords):
    if fitz is None:
        raise RuntimeError(f"PyMuPDF(fitz)ã®importã«å¤±æ•—ã—ã¾ã—ãŸã€‚å…ˆã« `pip install pymupdf` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„: {_FITZ_IMPORT_ERROR}")

    targets, label, (d_from, d_to), mode = select_target_folders(root_dir, target_spec)

    print("ãƒ«ãƒ¼ãƒˆ:", root_dir)
    print("å¯¾è±¡æŒ‡å®š:", target_spec, "mode=", mode, "from=", d_from, "to=", d_to)
    print("å¯¾è±¡ãƒ•ã‚©ãƒ«ãƒ€æ•°:", len(targets))
    print("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:", list(keywords))

    if not targets:
        raise FileNotFoundError("å¯¾è±¡æœŸé–“ã«è©²å½“ã™ã‚‹æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")

    results = []
    total_pdfs = 0
    hit_files = 0
    processed_pdfs = 0

    # äº‹å‰ã«ç·PDFæ•°ã‚’æ•°ãˆã¦ãŠãã€é€²æ—è¡¨ç¤ºã«åˆ©ç”¨ã™ã‚‹
    folder_pdf_counts = {}
    for d in targets:
        day_dir = os.path.join(root_dir, d)
        pdf_files = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]
        count = len(pdf_files)
        folder_pdf_counts[d] = count
        total_pdfs += count

    print("ç·PDFæ•°ï¼ˆæ¨å®šï¼‰:", total_pdfs)

    for idx, d in enumerate(targets, start=1):
        day_dir = os.path.join(root_dir, d)
        pdf_files = [f for f in os.listdir(day_dir) if f.lower().endswith(".pdf")]

        print(f"[{idx}/{len(targets)}] æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ {d} ã‚’å‡¦ç†ä¸­... (ã“ã®ãƒ•ã‚©ãƒ«ãƒ€å†…PDFæ•°: {len(pdf_files)})")

        folder_hits = 0

        for pdf_name in sorted(pdf_files):
            pdf_path = os.path.join(day_dir, pdf_name)
            processed_pdfs += 1

            hits_str, hit_pages_count, pages_str = extract_hits_pages_from_pdf(
                pdf_path, keywords, pages_sep=PAGES_SEPARATOR
            )

            if hits_str:
                hit_files += 1
                folder_hits += 1
                code = extract_code_from_pdf_filename(pdf_name)

                results.append(
                    {
                        "æ—¥ä»˜": d,
                        "ã‚³ãƒ¼ãƒ‰": code,
                        "PDFãƒ•ã‚¡ã‚¤ãƒ«å": pdf_name,
                        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": hits_str,
                        "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°": hit_pages_count,
                        "è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸": pages_str,
                    }
                )

            # 50ä»¶ã”ã¨ã«ã–ã£ãã‚Šé€²æ—ã‚’è¡¨ç¤ºï¼ˆãƒ’ãƒƒãƒˆã®æœ‰ç„¡ã«é–¢ä¿‚ãªãï¼‰
            if processed_pdfs % 50 == 0 or processed_pdfs == total_pdfs:
                print(f"  é€²æ—: {processed_pdfs}/{total_pdfs} ä»¶ã®PDFã‚’è§£ææ¸ˆã¿")

        # ãƒ•ã‚©ãƒ«ãƒ€å˜ä½ã®ãƒ’ãƒƒãƒˆä»¶æ•°ã‚’è¡¨ç¤º
        print(f"  â†’ æ—¥ä»˜ãƒ•ã‚©ãƒ«ãƒ€ {d} ã®ãƒ’ãƒƒãƒˆPDFæ•°: {folder_hits}")

    out_csv = f"{ANALYSIS_CSV_PREFIX}_{label}.csv"
    out_path = os.path.join(root_dir, out_csv)

    if not results:
        print("ãƒ’ãƒƒãƒˆã™ã‚‹PDFã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    df = pd.DataFrame(results)
    df_sorted = df.sort_values(by=["æ—¥ä»˜", "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°", "PDFãƒ•ã‚¡ã‚¤ãƒ«å"], ascending=[False, False, True])

    # æ¤œç´¢ã«ä½¿ç”¨ã—ãŸã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§ï¼ˆå…¨è¡ŒåŒã˜å€¤ï¼‰ã‚’åˆ—ã¨ã—ã¦è¿½åŠ 
    if keywords:
        keywords_summary = " / ".join(keywords)
    else:
        keywords_summary = ""

    df_sorted = df_sorted[["æ—¥ä»˜", "ã‚³ãƒ¼ãƒ‰", "PDFãƒ•ã‚¡ã‚¤ãƒ«å", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°", "è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸"]]
    df_sorted["æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ä¸€è¦§"] = keywords_summary

    archive_if_exists(out_path)
    df_sorted.to_csv(out_path, index=False, encoding="utf-8-sig")

    print("\nâœ… â‘¡A å®Œäº†ï¼ˆè§£æå°‚ç”¨ãƒ»ãƒšãƒ¼ã‚¸åˆ—ã‚ã‚Šï¼‰")
    print("è§£æPDFæ•°:", total_pdfs)
    print("ãƒ’ãƒƒãƒˆPDFæ•°:", hit_files)
    print("å‡ºåŠ›CSV:", out_csv)
    print("ä¿å­˜å…ˆ:", root_dir)


def run_distribute(root_dir: str, target_spec: str, stop_on_empty_meta: bool = True, use_local_link: bool = False):
    _, _, label, _ = parse_target_spec(target_spec)

    analysis_csv = f"{ANALYSIS_CSV_PREFIX}_{label}.csv"
    analysis_path = os.path.join(root_dir, analysis_csv)

    if not os.path.exists(analysis_path):
        raise FileNotFoundError(f"â‘¡Aã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {analysis_path}")

    hits_df = pd.read_csv(analysis_path, dtype=str).fillna("")
    hits_df.columns = [str(c).strip().replace("\ufeff", "") for c in hits_df.columns]

    required_in_hits = ["æ—¥ä»˜", "PDFãƒ•ã‚¡ã‚¤ãƒ«å", "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°", "è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸"]
    for c in required_in_hits:
        if c not in hits_df.columns:
            raise ValueError(f"â‘¡Açµæœã«å¿…è¦ãªåˆ—ãŒã‚ã‚Šã¾ã›ã‚“: {c}")

    dates = hits_df["æ—¥ä»˜"].astype(str).str.strip().tolist()
    tdnet_index, missing_csv_dates = build_tdnet_index_for_dates(root_dir, dates)

    if missing_csv_dates:
        print("âš  â‘ CSVãŒè¦‹ã¤ã‹ã‚‰ãªã„æ—¥ä»˜:", ", ".join(missing_csv_dates))

    results = []
    unmatched = 0
    alerts = 0

    for _, r in hits_df.iterrows():
        d = r["æ—¥ä»˜"].strip()
        pdf_raw = r["PDFãƒ•ã‚¡ã‚¤ãƒ«å"]
        pdf_key = norm_key(pdf_raw)

        meta = tdnet_index.get((d, pdf_key))
        if not meta:
            unmatched += 1
            continue

        empty = find_empty_fields(meta, REQUIRED_META_FIELDS)
        if empty:
            alerts += 1
            print(f"âš  å¿…é ˆé …ç›®ç©ºæ¬„: {d} / {pdf_raw} -> {empty}")
            if stop_on_empty_meta:
                raise RuntimeError("å¿…é ˆé …ç›®ãŒç©ºæ¬„ã®ãŸã‚å‡¦ç†ä¸­æ–­")

        # TDnetå´ã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºï¼ˆä¼šç¤¾å or HYPERLINKã®è¡¨ç¤ºãƒ†ã‚­ã‚¹ãƒˆ or PDFãƒ•ã‚¡ã‚¤ãƒ«åï¼‰
        title_link_tdnet = meta.get("è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰", "")
        display_text = meta.get("ä¼šç¤¾å", "") or pdf_raw
        m = re.match(r'=HYPERLINK\("[^"]*",\s*"([^"]*)"\)', str(title_link_tdnet))
        if m:
            display_text = m.group(1)

        # ãƒ­ãƒ¼ã‚«ãƒ«PDFã¸ã®ãƒªãƒ³ã‚¯å¼
        local_pdf_path = os.path.join(root_dir, d, pdf_raw)
        title_link_local = f'=HYPERLINK("{local_pdf_path}", "{display_text}")'

        # æ—¢å­˜äº’æ›ç”¨: --local-link ã®æœ‰ç„¡ã§ã€Œè¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã€åˆ—ã«ã©ã¡ã‚‰ã‚’å…¥ã‚Œã‚‹ã‹ã‚’åˆ‡ã‚Šæ›¿ãˆ
        out_title_link_main = title_link_local if use_local_link else title_link_tdnet

        results.append(
            {
                "æ—¥ä»˜": d,
                "æ™‚åˆ»": meta["æ™‚åˆ»"],
                "ã‚³ãƒ¼ãƒ‰": meta["ã‚³ãƒ¼ãƒ‰"],
                "ä¼šç¤¾å": meta["ä¼šç¤¾å"],
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰": out_title_link_main,
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_TDnetï¼‰": title_link_tdnet,
                "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰": title_link_local,
                "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰": r["ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰"],
                "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°": r["ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°"],
                "è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸": r["è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸"],
                "åˆ†é¡": meta["åˆ†é¡"],
                "URLï¼ˆç”Ÿï¼‰": meta["URLï¼ˆç”Ÿï¼‰"],
            }
        )

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å: é€šå¸¸ç‰ˆ(_sh) ã¨ è‡ªåˆ†ç”¨ãƒ­ãƒ¼ã‚«ãƒ«ãƒªãƒ³ã‚¯ç‰ˆ(_local_sh) ã‚’åˆ†ã‘ã‚‹
    suffix = "_local_sh.csv" if use_local_link else "_sh.csv"
    out_csv = f"{DISTRIBUTION_CSV_PREFIX}_{label}{suffix}"
    out_path = os.path.join(root_dir, out_csv)

    archive_if_exists(out_path)
    out_df = pd.DataFrame(results)

    cols_order = [
        "æ—¥ä»˜",
        "æ™‚åˆ»",
        "ã‚³ãƒ¼ãƒ‰",
        "ä¼šç¤¾å",
        "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰",
        "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_TDnetï¼‰",
        "è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯_ãƒ­ãƒ¼ã‚«ãƒ«ï¼‰",
        "ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰",
        "ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸æ•°",
        "è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãƒšãƒ¼ã‚¸",
        "åˆ†é¡",
        "URLï¼ˆç”Ÿï¼‰",
    ]
    final_cols = [c for c in cols_order if c in out_df.columns]
    out_df = out_df[final_cols]
    out_df.to_csv(out_path, index=False, encoding="utf-8-sig")

    print("\nâœ… â‘¡B å®Œäº†ï¼ˆé…å¸ƒç”¨CSVä½œæˆï¼‰")
    print("å‡ºåŠ›:", out_csv)
    print("çªåˆé™¤å¤–ä»¶æ•°:", unmatched)
    print("ç©ºæ¬„ã‚¢ãƒ©ãƒ¼ãƒˆä»¶æ•°:", alerts)


def parse_args():
    p = argparse.ArgumentParser(description="â‘¡ ãƒ•ãƒªãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆanalyze/distribute/titleï¼‰")
    sub = p.add_subparsers(dest="cmd", required=True)

    p_an = sub.add_parser("analyze", help="â‘¡A: PDFå…¨æ–‡æ¤œç´¢ï¼ˆTDnetã‚¢ã‚¯ã‚»ã‚¹ãªã—ï¼‰")
    p_an.add_argument("--save-root", default=DEFAULT_SAVE_ROOT, help="ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆï¼ˆâ‘ ã®å‡ºåŠ›å…ˆï¼‰")
    p_an.add_argument("--target", default=DEFAULT_TARGET_SPEC, help="YYYYMMDD / YYYYMM / 'YYYYMMDD YYYYMMDD'")
    p_an.add_argument("--keywords", nargs="+", default=DEFAULT_SEARCH_KEYWORDS, help="æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰")

    p_di = sub.add_parser("distribute", help="â‘¡B: â‘¡Açµæœã¨â‘ ã®CSVã‚’çªåˆã—ã¦é…å¸ƒç”¨CSVã‚’ä½œæˆ")
    p_di.add_argument("--save-root", default=DEFAULT_SAVE_ROOT, help="ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆï¼ˆâ‘ ã®å‡ºåŠ›å…ˆï¼‰")
    p_di.add_argument("--target", default=DEFAULT_TARGET_SPEC, help="â‘¡Aã¨åŒã˜TARGET_SPECï¼ˆlabelä¸€è‡´ç”¨ï¼‰")
    p_di.add_argument("--stop-on-empty-meta", action="store_true", default=True, help="å¿…é ˆãƒ¡ã‚¿ãŒç©ºæ¬„ãªã‚‰ã‚¨ãƒ©ãƒ¼ã§åœæ­¢ï¼ˆæ—¢å®š: åœæ­¢ï¼‰")
    p_di.add_argument("--no-stop-on-empty-meta", dest="stop_on_empty_meta", action="store_false", help="å¿…é ˆãƒ¡ã‚¿ãŒç©ºæ¬„ã§ã‚‚ç¶™ç¶š")
    p_di.add_argument(
        "--local-link",
        action="store_true",
        help="è¡¨é¡Œï¼ˆãƒªãƒ³ã‚¯ï¼‰ã‚’TDnetã§ã¯ãªããƒ­ãƒ¼ã‚«ãƒ«PDFã¸ã®ãƒªãƒ³ã‚¯ã«ã™ã‚‹ï¼ˆè‡ªåˆ†ç”¨ï¼‰",
    )

    p_title = sub.add_parser("title", help="è¡¨é¡Œï¼ˆã‚¿ã‚¤ãƒˆãƒ«ï¼‰ã«å¯¾ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢ï¼ˆPDFæœ¬æ–‡ã¯èª­ã¾ãªã„é«˜é€Ÿç‰ˆï¼‰")
    p_title.add_argument("--save-root", default=DEFAULT_SAVE_ROOT, help="ä¿å­˜å…ˆãƒ«ãƒ¼ãƒˆï¼ˆâ‘ ã®å‡ºåŠ›å…ˆï¼‰")
    p_title.add_argument("--target", default=DEFAULT_TARGET_SPEC, help="YYYYMMDD / YYYYMM / 'YYYYMMDD YYYYMMDD'")
    p_title.add_argument(
        "--keywords",
        nargs="+",
        required=True,
        help="è¡¨é¡Œã«å«ã¾ã‚Œã¦ã„ã¦ã»ã—ã„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆéƒ¨åˆ†ä¸€è‡´ãƒ»è¤‡æ•°æŒ‡å®šå¯ï¼‰",
    )

    return p.parse_args()


def main():
    args = parse_args()
    root_dir = str(Path(args.save_root))

    if args.cmd == "analyze":
        run_analyze(root_dir=root_dir, target_spec=args.target, keywords=args.keywords)
    elif args.cmd == "distribute":
        run_distribute(
            root_dir=root_dir,
            target_spec=args.target,
            stop_on_empty_meta=args.stop_on_empty_meta,
            use_local_link=getattr(args, "local_link", False),
        )
    elif args.cmd == "title":
        run_title_search(root_dir=root_dir, target_spec=args.target, keywords=args.keywords)
    else:
        raise ValueError("ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚")


if __name__ == "__main__":
    main()